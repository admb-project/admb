% $Id$
%
% Author: Hans Skaug
% Copyright (c) 2008 Regents of the University of California
%

\documentclass[12pt,letter,reqno]{book}
\usepackage{amssymb}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{harvard,makeidx,hyperref,here}
\usepackage{harvard,makeidx,hyperref,float}
\usepackage{graphicx}
\usepackage{xcolor}

\usepackage{listings}

\lstset{language=C++, numbers=none, basicstyle={\ttfamily}, 
%keywordstyle=\color{PrimColDark}\bfseries, 
%stringstyle=\color{AltColDark}, fontadjust=true, xleftmargin=6pt, 
%xrightmargin=6pt, breaklines=true, frame=single, 
%backgroundcolor=\color{Gainsboro}, 
%rulecolor=\color{PrimColLight}, 
%framerule=0pt, framesep=6pt, extendedchars=true, showspaces=false, 
columns=flexible}

%\textheight23cm
%\hoffset-1.3cm
%\voffset-1cm
%\textwidth16cm
%\parindent 0.7cm

\oddsidemargin=0.25in
\evensidemargin=0.25in
\textheight=8in
\textwidth=5.5in
%\hoffset=-0.0in
%\headheight=-0.0truein
%\headsep=0pt
%\topmargin=-.35truein
%\topskip=-2.75truein

\makeindex

\begin{document}

\title{ Random effects in AD Model Builder\\~ \\
	ADMB-RE user guide}
\author{admb-project.org}

\maketitle

\centerline{\LARGE License}

\input ../../../LICENSE

\newpage

\tableofcontents

\chapter*{Preface}

A few comments about notation: Important points are emphasized with a star
\begin{itemize}
\item[$\bigstar$] like this.
\end{itemize}

Please submit all comments and complaints by email to users@admb-project.org.

\chapter{Introduction}

This document is a user's guide to random effects modelling in AD Model Builder (ADMB).
Random effects is feature of ADMB, in the same way as profile likelihoods are, 
but sufficiently complex to merit a separate user manual. The work on the random effects ``module" (ADMB-RE) 
started around 2003. The pre-existing part of ADMB (and its evolution) is referred to as ``ordinary" ADMB
in the following. This manual refers to version 9.0.x of ADMB and (ADMB-RE).

Before you start with random effects it is recommended that you have some experience
with ordinary ADMB. This manual tries to be self contained, but it is clearly an advantage if
you have written (and successfully run) a few tpl-files. Ordinary ADMB is described in 
the ADMB manual~\cite{admb_manual} which is available from admb-project.org.
If you are new to ADMB, but have experience with C++ (or a similar programming language)
you may benefit from taking a look at the quick references in Appendix~\ref{sec:quick}.

ADMB-RE is very flexible. The term ``random effect" seems to indicate that it only can handle mixed-effect
regression type models, but this is very misleading. ``Latent variable" module would have
been a more precise term. It can be argued ADMB-RE is the most flexible latent variable framework around.
All the mixed model stuff in software packages such that R, Stata, SPSS, etc.~allow only very specific models
to be fit, and it is impossible to change the distribution of the random effects, say, if you wanted to do that.
The NLMIXED macro in SAS is more flexible, but cannot handle state-space models or models with crossed
random effects. WinBUGS is the only exception, which with it ability to handle discrete
latent variables, is a bit more flexible than ADMB-RE. However, WinBUGS does all its computations using MCMC exclusively,
while ADMB-RE lets the user choose between maximum likelihood estimation (which in general is much faster) and MCMC.

An important part of the ADMB-RE documentation is the example collection which is described in Appendix \ref{sec:example_collection}.
As with the example collections for ADMB and AUTODIF, you will find fully worked examples including data and code for the model. 
The examples have been selected to illustrate the various aspects of ADMB-RE, and are frequently referred to troughout this manual.


\section{Summary of features}
Why use AD Model Builder for creating nonlinear random effects models? The answer consists of three words --
flexibility, speed and accuracy. To illustrate these points a number of examples comparing ADMB-RE with two
existing packages NLME which runs on R and Splus, and WinBUGS. In general NLME is rather fast and it is good for
the problems for which it was designed, but it is quite inflexible. What is needed is a tool with at least the
computational power of NLME but the flexibility to deal with arbitrary nonlinear random effects models. In
section~\ref{lognormal} we consider a thread from the R user list where a discussion about extending a model to
use random effects which had a log-normal rather than normal distribution took place. This appeared to be quite
difficult. With ADMB-RE this change takes one line of code. WinBUGS on the other hand is very flexible and many
random effects models can be easily formulated in it. However, it can be very slow and it is necessary to adopt
a Bayesian perspective which may be a problem for some applications. 
A model which runs 25~times faster under ADMB than under WinBUGS may be found in~\ref{sec:logistic_example}.

\paragraph{Model formulation}

With ADMB you can formulate and fit a large class of nonlinear statistical
models. With ADMB-RE you can include random effects in your model. Examples of such models include:
\begin{itemize}
\item Generalized linear mixed models (logistic and Poisson regression).
\item Nonlinear mixed models (growth curve models, pharmacokinetics).
\item State space models (nonlinear Kalman filters).
\item Frailty models in survival analysis.
\item Nonparametric smoothing.
\item Semiparametric modelling.
\item Frailty models in survival analysis.
\item Bayesian hierarchical models.
\item General nonlinear random effects models (fisheries catch-at-age
models).
\end{itemize}
You formulate the likelihood function in a template file, using a language that resembles C++. The file is
compiled into an executable program (Linux or Windows). The whole C++ language is to your disposal, giving you
great flexibility with respect to model formulation.

\paragraph{Computational basis of ADMB-RE}

\begin{itemize}
\item Hyper-parameters (variance components etc.)~estimated by maximum likelihood.
\item Marginal likelihood evaluated by the Laplace approximation or importance sampling.
\item Exact derivatives calculated using Automatic Differentiation.
\item Sampling from the Bayesian posterior using MCMC (Metropolis-Hastings algorithm).
\item Most features of ordinary ADMB (matrix arithmetic and standard errors, etc.) are available.
\item Sparse matrix libraries useful for Markov random fields and crossed random effects are available.
\end{itemize}

\paragraph{The strengths of ADMB-RE}

\begin{itemize}
\item \textit{Flexibility}: You can fit a large variety of models within a single framework.
\item \textit{Convenience}: Computational details are transparent. Your only
responsibility is to formulate the loglikelihood
\item \textit{Computational efficiency}: ADMB-RE is up to 50 times faster
than WinBUGS.
\item \textit{Robustness}: With exact derivatives you can fit highly nonlinear models.
\item \textit{Convergence diagnostic}: The gradient of the likelihood
function provides a clear convergence diagnostic.
\end{itemize}

\paragraph{Program interface}

\begin{itemize}
\item\textit{Model formulation}: You fill in a C++ based template
using your favorite text editor.

\item \textit{Compilation}: You turn your model into an executable program using
a  C++ compiler (which you need to install separately).

\item\textit{Platforms}: Windows and Linux
\end{itemize}

\paragraph{How to obtain ADMB-RE}

ADMB-RE is a module for ADMB. Both can be obtained from
admb-project.org

\chapter{The language and the program}

\section{What is ordinary ADMB?}

ADMB is a software package for doing parameter estimation in nonlinear
models. It combines a flexible mathematical modelling language (built on
C++) with a powerful function minimizer (based on Automatic
Differentiation). The following features of ADMB make it very useful for
building and fitting nonlinear models to data:

\begin{itemize}
\item Vector-matrix arithmetic, vectorized operations for common mathematical functions.
\item Read and write vector and matrix objects to file.
\item Fit the model is a stepwise manner (with `phases'), where more and more parameters become active in the minimization.
\item Calculate standard deviations of arbitrary functions of the model parameters by the `delta method'.
\item MCMC sampling around the posterior mode.
\end{itemize}
To use random effects in ADMB it is recommended that you have some experience in writing ordinary ADMB programs.
In this sections we review, for the benefit of the reader without this experience, the basic constructs of ADMB.

Model fitting with ADMB has three stages: 1) Model formulation, 2) Compilation and 3) Program execution.
The model fitting process is typically iterative: After having looked at the output from stage 3) one goes back to
stage 1) and modifies some aspect of the program.

\paragraph{Writing an ADMB program}
\index{tpl-file!writing}To fit a statistical model to data we must carry out certain fundamental tasks, such as
reading data from file, declaring the set of parameters that should be estimated, and finally we must give a
mathematical description of the model. In ADMB you do all of this by filling in a template, which is an ordinary
text file with the file-name extension `.tpl' (and hence the template file is known as the tpl-file). You
therefore need a text editor, such as 'vi' under Linux or 'Notepad' under Windows, to write the tpl-file. The
first tpl-file to which the reader of the ordinary ADMB manual is exposed is \texttt{simple.tpl} (listed
in~Section~\ref{sec:code example} below). We shall use \texttt{simple.tpl} as our generic tpl-file, and we shall
see that introduction of random effects only requires small changes to the program.

A tpl-file is divided into a number of `sections', each representing one of the fundamental tasks mentioned
above. The required sections are:
\begin{center}
\begin{tabular}{ll}
\textbf{Name} & \textbf{Purpose} \\ \hline
\texttt{DATA\_SECTION} & Declare `global' data objects; initialization from file \\
\texttt{PARAMETER\_SECTION} & Declare independent parameters \\
\texttt{PROCEDURE\_SECTION} & Specify model and objective function in C++
\end{tabular}
\end{center}
More details are given when we later look at \texttt{simple.tpl}, and a quick reference
card is available in Appendix~\ref{sec:quick}.

\paragraph{Compiling an ADMB program}

\index{tpl-file!compiling}After having finished writing \texttt{simple.tpl},
we want to convert it into an executable program. This is done in a
DOS-window under Windows, and in an ordinary terminal window under Linux. To
compile \texttt{simple.tpl}, we would under both platforms give the command:
\begin{lstlisting}
    $ admb -r simple
\end{lstlisting}
Here, '\texttt{\$}' is the command line prompt (which may be a different symbol on your computer), and
\texttt{-r} is an option telling the program \texttt{admb} that your model contains random effects. The program
\texttt{admb} accepts another option \texttt{-s} which produces the `safe' (but slower) version of the
executable program. The \texttt{-s} option should be used in a debugging phase, but it should be skipped when
the final production version of the program is generated.

The compilation process really consists of two steps: first \texttt{simple.tpl} is converted to a C++ program by
a preprosessor called \texttt{tpl2rem} in the case of ADMB-RE and \texttt{tpl2cpp} in the case of ordinary ADMB (Appendix~\ref{sec:quick}). 
An error message from \texttt{tpl2rem} consists of a 
single line of text, with a reference to the line in the tpl-file where the error occurs. 
If successful, the first compilation step results in
the C++ file \texttt{\ simple.cpp}. In the second step \texttt{simple.cpp} is compiled and linked using an
ordinary C++ compiler (which is not part of ADMB). Error messages during this phase typically consist of long
printouts, with references to line numbers in \texttt{simple.cpp}. To track down syntax errors it may
occasionally be useful to look at the content of \texttt{simple.cpp}. When you understand what is wrong in
\texttt{simple.cpp} you should go back and correct \texttt{simple.tpl} and re-enter the command \texttt{admb -r
simple}. When all errors have been removed, the result will be an executable file, which is called
\texttt{simple.exe} under Windows or \texttt{simple} under Linux. The compilation process is
illustrated in Figure~\ref{fig:compiling}.

\paragraph{Running an ADMB-program}
\index{tpl-file!compiling}The executable program is run in the same window as it was compiled. Note that data
are not usually part of the ADMB program (\texttt{simple.tpl}). Instead, data are being read from a file with
the file name extension `.dat' (\texttt{simple.dat}). This brings us to the naming convention used by ADMB programs for
input and output files: The executable automatically infers file names by adding an extension to its own
name. The most important files are:
\begin{center}
\begin{tabular}{lll}
& \textbf{File name} & \textbf{Contents} \\ \hline
Input & \texttt{simple.dat} & Data for the analysis \\
& \texttt{simple.pin} & Initial parameter values \\ \hline
Output & \texttt{simple.par} & Parameter estimates \\
& \texttt{simple.std} & Standard deviations \\
& \texttt{simple.cor} & Parameter correlations
\end{tabular}
\end{center}
You can use command line options to modify the behavior of the program at runtime. The available command line options can be
listed by typing:
\begin{lstlisting}

    $ simple -?

\end{lstlisting}
(or whatever your executable is called). The command line options that
are specific to ADMB-RE are listed in Appendix~\ref{sec:command_line_options}, and are discussed in detail under the various sections. An
option you probably will like to use during an experimentation phase is \texttt{-est}, which turns off
calculation of standard deviations, and hence reduces the running time of the program.

\paragraph{ADMB-IDE: Easy and efficient user interface}
The graphical user interfase to ADMB by Arni Magnusson simplifies the process
of building and running the model, especially for the beginner~\cite{admb_news_july09}. Among other things, it provides syntax highlighting
and links error messages from the C++ compiler to the \texttt{.cpp} file.

\paragraph{Initial values}
The initial values can be provided in different ways (see the ordinary ADMB manual). Here
we only describe the \texttt{.pin} file approach. The \texttt{.pin} file should contain
legal values (within the bounds) for all the parameters, including the random effects.
The values must be given in the same order as the parameters are defined in the \texttt{.tpl} file .
The easiest way of generating a \texttt{.pin} file with the right structure is to 
first run the program with an \texttt{-maxfn 0} option (for this you do not need a \texttt{.pin} file)
and copy the resulting \texttt{.p01} file into \texttt{.pin} file, and edit it to provide the correct numeric values.
More information about what initial values for random effects really means is given in Section~\ref{sec:hood}.

\section{Why random effects?}

Many people are familiar with the method of least squares for parameter estimation. Far fewer know about random
effects modeling. The use of random effects requires that we adopt a statistical point of view, where the sum of
squares is interpreted as being part of a likelihood function. When data are correlated, the method of least
squares is sub-optimal, or even biased. But relax, random effects come to rescue! \index{random effects}

The classical motivation of random effects is:

\begin{itemize}
\item To create parsimonious and interpretable correlation structures.

\item To account for additional variation or overdispersion.
\end{itemize}
We shall see, however, that random effects are useful in a much wider context. 
For instance, in non-parametric smoothing (\ref{}).

\paragraph{Statistical prerequisites}
To use random effects in ADMB you must be familiar with the notion of a random variable, and in particular with
the normal distribution. In case you are not, please consult a standard textbook in statistics. The notation
$u\sim N(\mu ,\sigma ^{2})$ is used throughout this manual, and means that $u$ has a normal (Gaussian)
distribution with expectation $\mu $ and variance $\sigma ^{2}$. The distribution placed on the random effects
is called the 'prior', which is a term borrowed from Bayesian statistics.

A central concept that originates from generalized linear models is that of a linear predictor. Let
$x_{1},\ldots ,x_{p}$ denote observed covariates (explanatory variables), and let $\beta _{1},\ldots ,\beta
_{p}$ be the corresponding regression parameters to be estimated. Many of the examples in this manual involve a
linear predictor $\eta_{i}=\beta_{1}x_{1,i}+\cdots +\beta_{p}x_{p,i}$, which we also will write on vector form as
$\mathbf{\eta}=\mathbf{X\beta }$. \index{linear predictor}

\paragraph{Frequentist or Bayesian statistics?}
A pragmatic definition of a frequentist is a person who prefers to estimate parameters by the method of maximum likelihood.
Similarly, a Bayesian is a person who use MCMC techniques to generate samples from the posterior distribution
(typically with noninformative priors on hyper-parameters), and from these samples generates some summary
statistic such as the posterior mean. With its \texttt{-mcmc} runtime option ADMB lets you switch freely between
the two worlds. The approaches complement each other rather than being competitors. A maximum
likelihood fit (point estimate + covariance matrix) is a step-1 analysis. For some purposes step-1 is
sufficient. In other situations, one may want to see posterior distributions for the parameters.
In such situations the established covariance matrix (inverse Hessian of the log-likelihood) is used by ADMB 
to implement an efficient Metropolis-Hastings algorithm (which you invoke with \texttt{-mcmc}).

\paragraph{A simple example}
We use the \texttt{simple.tpl} example from the ordinary ADMB manual to
exemplify the use of random effects. The statistical model underlying this
example is the simple linear regression
\[
Y_i=ax_i+b+\varepsilon_i,\qquad i=1,\ldots ,n,
\]
where $Y_i$ and $x_i$ are the data, $a$ and $b$ are the unknown parameters
to be estimated, and $\varepsilon_i\sim N(0,\sigma ^{2})$ is an error term.

Consider now the situation that we do not observe $x_i$ directly, but rather
we observe
\[
X_i=x_i+e_i,
\]
where $e_i$ is a measurement error term. This situation frequently occurs in
observational studies, and is known as the `error in variables' problem.
Assume further that $e_i\sim N(0,\sigma_{e}^{2})$, where $\sigma_{e}^{2}$ is
the measurement error variance. For reasons discussed below, we shall assume that we know
the value of $\sigma_e$, so we shall pretend that $\sigma_e=0.5$.

Because $x_i$ is not observed, we model it as a random effect with $x_i\sim
N(\mu ,\sigma_{x}^{2})$. In ADMB-RE you are allowed to make such definitions
through the new parameter type \texttt{random\_effects\_vector}.
\index{random effects!random effects vector} (There is also a \texttt{random\_effects\_matrix}
which allows you to define a matrix of random effects).
\index{random effects!random effects matrix}

\begin{enumerate}
\item Why do we call $x_i$ a random effect, while we do not use this term
for $X_i$ and $Y_i$ (though they clearly are 'random')? The point is that $X_i$ and $Y_i$ are observed directly,
while $x_i$ is not. The term 'random effect' comes from regression analysis, where it means a random regression
coefficient. In a more general context 'latent random variable' is probably a better term.

\item The unknown parameters in our model are: $a$, $b$, $\mu $, $\sigma $, $\sigma_{x}$ and $x_{1},\ldots ,x_{n}$.
We have agreed to call $x_{1},\ldots,x_{n}$ random effects. The rest of the parameters are called
hyper-parameters. Note that we place no prior distribution on the hyper-parameters.

\item Random effects are integrated out of the likelihood, while
hyper-parameters are estimated by maximum likelihood~\index{hyper-parameter}. This approach is often called
`empirical Bayes', and will be considered a frequentist method by most people. There is however nothing
preventing you from making it `more Bayesian' by putting priors (penalties) on the hyper-parameters.

\item A statistician will say ''this model is nothing but a bivariate
Gaussian distribution for $(X,Y)$, and we don't need any random effects in
this situation''. This is formally true, because we could work out the covariance
matrix of $(X,Y)$ by hand and fit the model using ordinary ADMB. This
program would probably run much faster, but it would have
taken us longer to write the code without declaring $x_i$ to be of type
\texttt{random\_effects\_vector}. But, more important is that random effects
can be used also in non-Gaussian (nonlinear) models where we are unable to
derive an analytical expression for the distribution of $(X,Y)$.

\item Why didn't we try to estimate $\sigma_{e}$? Well, let us count the
parameters in the model: $a$, $b$, $\mu $, $\sigma $, $\sigma_{x}$ and $\sigma_{e}$; totally six parameters. We
know that the bivariate Gaussian distribution has only five parameters (the means of $X$ and $Y$ and three free parameters in
the covariate matrix). Thus, our model is not identifiable if we also try to estimate $\sigma_{e}$. Instead, we
pretend that we have estimated $\sigma_{e}$ from some external data source. This example illustrates a general
point in random effects modelling: you must be careful to make sure that the model is identifiable!\quad
\end{enumerate}

\section{A code example\label{sec:code example}}

Here is the random effects version of \texttt{simple.tpl}:
\begin{lstlisting}

    DATA_SECTION
     init_int nobs
     init_vector Y(1,nobs)
     init_vector X(1,nobs)

    PARAMETER_SECTION
     init_number a
     init_number b
     init_number mu
     vector pred_Y(1,nobs)
     init_bounded_number sigma_Y(0.000001,10)
     init_bounded_number sigma_x(0.000001,10)
     random_effects_vector x(1,nobs)
     objective_function_value f

    PROCEDURE_SECTION               // This section is pure C++
     f = 0;
     pred_Y=a*x+b;                  // Vectorized operations

     // Prior part for random effects x
     f += -nobs*log(sigma_x) - 0.5*norm2((x-mu)/sigma_x);

     // Likelihood part
     f += -nobs*log(sigma_Y) - 0.5*norm2((pred_Y-Y)/sigma_Y);
     f += -0.5*norm2((X-x)/0.5);

     f *= -1;  // ADMB does minimization!

\end{lstlisting}

\paragraph{Comments}

\begin{enumerate}
\item Everything following '\texttt{//}' is a comment.

\item In the \texttt{DATA\_SECTION}, variables with a \texttt{init\_} in
front of the data type are read from file.

\item In the \texttt{PARAMETER\_SECTION}
\begin{itemize}
\item Variables with a \texttt{init\_}
	in front of the data type are the hyper-parameters, i.e. the parameters to
	be estimated by maximum likelihood.
\item \texttt{random\_effects\_vector} defines the random effect vector (there
    is also a \texttt{random\_effects\_matrix}). There can be more than one
    such object, but they must all be defined after the hyper-parameters, otherwise
    you will get an error message from the preprocessor \texttt{tpl2rem}.
\item Objects that neither are hyper-parameters or random effects are ordinary programming variables 
	that can be used in the \texttt{PROCEDURE\_SECTION}. For instance, we can assign a value to the
	vector \texttt{pred\_Y}.
\item The objective function should be defined as the last variable.
\end{itemize}

\item The \texttt{PROCEDURE\_SECTION} basically consists of standard C++ code which primary purpose 
	is to calculate the value of the objective function. 
\begin{itemize}
\item Variables defined in \texttt{DATA\_SECTION} and \texttt{PARAMETER\_SECTION} may be used.
\item Standard C++ functions as well as special ADMB functions, such as \texttt{norm2(x)} 
	(which calculates $\sum x_i^2$), may be used.
\item Often the operations are vectorized, as in the case of \texttt{simple.tpl}
\item The objective function should be defined as the last variable.
\item ADMB does minimization, rather than optimization. Thus, the sign of
the loglikelihood function \texttt{f} is changed in the last line of the
code.
\end{itemize}
\end{enumerate}

\paragraph{Parameter estimation}

We learned above that hyper-parameters are estimated but maximum likelihood, but what if we also are interested
in the value of the random effects? For this purpose ADMB-RE offers an `empirical Bayes' approach, which
involves fixing the hyper-parameters at their maximum likelihood estimates, and treating the random effects as
the parameters of the model. ADMB-RE automatically calculates `maximum posterior' estimates of the random
effects for you. Estimates of both hyper-parameters and random effects are written to \texttt{simple.par}.

\section{The flexibility of ADMB-RE\label{lognormal}}

Say that you doubt the distributional assumption $x_i\sim N(\mu ,\sigma _{x}^{2})$ that was made in
\texttt{simple.tpl}, and that you want to check if a skewed distribution gives a better fit. You could for
instance take
\[
x_i=\mu +\sigma_{x}\exp (z_i),\qquad z_i\sim N(0,1).
\]
Under this model the standard deviation of $x_i$ is proportional to, but not directly equal to $\sigma_{x}$. It is
easy to make this modification in \texttt{simple.tpl}. In the \texttt{PARAMETER\_SECTION} we replace the
    declaration of \texttt{x} by
\begin{lstlisting}

    vector x(1,nobs)
    random_effects_vector z(1,nobs)

\end{lstlisting}

and in the \texttt{PROCEDURE\_SECTION} we replace the prior on \texttt{x} by
\begin{lstlisting}

    f = - 0.5*norm2(z);
    x = mu + sigma_x*exp(z);

\end{lstlisting}

This example shows one of the strengths of ADMB-RE: it is very easy to
modify models. In principle you can implement any random effects model you
can think of, but as we shall discuss later, there are limits to the number
of random effects you can declare.


\chapter{Random effects modeling}
This chapter describes all ADMB-RE features, except those related to ``separability"  which are
dealt with in Chapter~\ref{separability}. Separability, or the Markov property as it
is called in statistics, is a property possessed by many model classes allows ADMB-RE
to generate more efficient executable programs. However, most ADMB-RE concepts and techniques are better learned 
and understood without introducing separability. Throughout much of this chapter we will refer to the 
program \texttt{simple.tpl} from Section~\ref{sec:code example}.


\section{The objective function}
As with ordinary ADMB the user specifies an objective function in terms of data and parameters, but in ADMB-RE
the objective function must have the interpretation of being a (negative) log-likelihood. One typically has got
a hierarchical specification of the model, where at the top layer data are assumed to have a certain probability
distribution conditionally on the random effects (and the hyper-parameters), and at the next level the random
effects are assigned a prior distribution (typically normal). Because conditional probabilities are
multiplied to yield the joint distribution of data and random effects, the objective function becomes a sum of
(negative) log-likelihood contributions, and the following rule applies
\begin{itemize}
\item[$\bigstar$] 
The order in which the different loglikelihood contributions are added to the objective function does not matter.
\end{itemize}
An addition to this rule is that all programming variables have got their value assigned before
they enter in a prior or a likelihood expression. WinBUGS users must take care when porting their programs to 
ADMB because this is not required in WinBUGS.

The reason why the {\bf negative} log-likelihood is used is that ADMB 
for historical reasons does minimization (as opposed to maximization). In complex models, 
with contributions to the log-likelihood coming from a variety of data sources and random effects priors, 
it is recommended that you collect the contributions
to the objective function using the \texttt{-=} operator of C++, i.e.
\begin{lstlisting}
     f -= -nobs*log(sigma_x) - 0.5*norm2((x-mu)/sigma_x);
\end{lstlisting}
By using \texttt{-=} instead of \texttt{+=} you do not have to change the sign of every likelihood expression,
which would be a likely source of error. 
When non of the advanced features of Chapter~\ref{separability} are used, you are allowed to
switch the sign of the objective function at the end of the program
\begin{lstlisting}
     f *= -1;  // ADMB does minimization!
\end{lstlisting}
so that in fact \texttt{f} can hold the value of the log-likelihood until the last line of the program.

It is OK to ignore constant terms ($0.5\log(2\pi)$ for the normal distribution) as we did in \texttt{simple.tpl}.
This only affects the objective function value, not any other quantity reported in the \texttt{.par} and \texttt{.std}
(not even the gradient value).

\section{The random effects distribution (prior)}
In \texttt{simple.tpl} we declared $x_{1},\ldots ,x_{n}$ to be of type \texttt{random\_effects\_vector}. This
statement tells ADMB that $x_{1},\ldots ,x_{n}$ should be treated as random effects (i.e.~be the targets for the
Laplace approximation), but it does not say anything about which distribution the random effects should have. 
We assumed that $x_i\sim N(\mu ,\sigma_{x}^{2})$, and (without saying it explicitly)
that the $x_i$'s were statistically independent. We know that the corresponding prior contribution to the
loglikelihood is
\[
-n\log (\sigma_{x})-\frac{1}{2\sigma_x^2}\sum_{i=1}\left( x_i-\mu \right) ^{2}.
\]
with ADMB implementation
\begin{lstlisting}

    f += -nobs*log(sigma_x) - 0.5*norm2((x-mu)/sigma_x);

\end{lstlisting}
Both the assumption about independence and normality can be generalized, as we shortly will do,
but first we introduce a transformation technique that forms the basis for much of which follows later.

\paragraph{Scaling of random effects}
A frequent source of error when writing ADMB-RE programs is that priors get
wrongly specified. The following trick can make the code easier to read, and
has the additional advantage of being numerically stable for small values of
$\sigma_{x}$. From basic probability theory we know that if $u\sim N(0,1)$,
then $x=\sigma_{x}u+\mu$ will have a $N(\mu ,\sigma_{x}^{2})$ distribution.
The corresponding ADMB code would be
\begin{lstlisting}
    f += - 0.5*norm2(u);
    x = sigma_x*u + mu;
\end{lstlisting}
(This, of course, requires that we change the type of \texttt{x} from \texttt{\ random\_effects\_vector} to
\texttt{vector}, and that \texttt{u} is declared as a \texttt{random\_effects\_vector}.) 

The trick here was to start with a $N(0,1)$ distributed random effect \texttt{u} and to generate random
effects \texttt{x} with another distribution. This is a special case of a transformation. 
Had we used a non-linear transformation we would have
got a \texttt{x} with a non-gaussian distribution. The way we obtain correlated random
effects is also transformation based. However, as we shall see in Chapter~\ref{separability} transformation
may ``break" the separability of the model, so there are limitations to what transformations can do for you.


\section{Correlated random effects\label{sec:correlated}} \index{random effects!correlated}
In some situation you will need correlated random effects, and as part of your problem
you may want to estimate the elements of the covariance matrix. A typical example is mixed
regression where the intercept random effect ($u_{i}$) is correlated with the slope random effect ($v_{i}$),
\[
  y_{ij}=(a+u_{i})+\left(b+v_{i}\right)x_{ij}+\varepsilon_{ij}.
\]
(If you are not familiar with the notation, please consult an introductory book on mixed regression, such 
\citeasnoun{pinh:bate:2000}.) In this case we can define correlation matrix
\[
 C=\left[\begin{array}{cc}
 1 & \rho\\
 \rho & 1\end{array}\right],
\]
and we want to estimate $\rho$ along with the variances of $u_{i}$ and $v_{i}$.
Here it is trivial to ensure that $C$ is positive definite, by requiring $-1<\rho<1$, but in higher dimensions
this issue requires more careful consideration. 
 
To ensure that $C$ is positive definite you can parameterize the problem in terms of
the Cholesky factor $L$, i.e.~$C=LL^\prime$, where $L$ is a lower diagonal matrix with positive diagonal elements.
There are $q(q-1)/2)$ free parameters (the non-zero elements of $L$) to be estimated, where $q$ is the dimension of $C$.
Since $C$ is a correlation matrix we must ensure that its diagonal elements are unity. An example with $q=4$ is
\begin{lstlisting}
PARAMETER_SECTION
  matrix L(1,4,1,4)                    // Cholesky factor
  init_vector a(1,6)                   // Free parameters in C
  init_bounded_vector B(1,4,0,10)      // Standard deviations
  
PROCEDURE_SECTION

  int k=0;
  L(1,1) = 1.0;
  for(i=2;i<=4;i++)
  {
    L(i,i) = 1.0;
    for(j=1;j<=i-1;j++)
      L(i,j) = a(k++);
    L(i)(1,i) /= norm(L(i)(1,i));     // Ensures that C(i,i) = 1
  }
\end{lstlisting}
Given the Cholesky factor $L$, we can proceed in different directions. One option is to use the same 
transformation-of-variable technique as above: 
Start out with a vector $u$ of independent $N(0,1)$ distributed random effects. Then, the vector
\begin{lstlisting}
  x = L*u;
\end{lstlisting}
has correlation matrix $C=LL^\prime$. Finally, we multiply each component of \texttt{x} by the 
appropriate standard deviation:
\begin{lstlisting}
  y = elem_prod(x,sigma);
\end{lstlisting}


\paragraph{Large structured covariance matrices} 
In some situations, for instance in spatial models, $q$ will be large ($q=100$, say). Then
it is better to use the approach outlined in Section~\ref{gaussianprior}.

\section{Non-Gaussian random effects}
Usually, the random effects will have a Gaussian distribution, but technically speaking there is nothing
preventing you from replacing the normality assumption, such as
\begin{lstlisting}
     f -= -nobs*log(sigma_x) - 0.5*norm2((x-mu)/sigma_x);
\end{lstlisting}
with a log gamma density, say. It can however be expected that the
Laplace approximation will be less accurate when you move away from
 normal priors. Hence, you should instead use the transformation trick that
we learned earlier, but now with a non-linear transformation.
A simple example of this yielding a log-normal prior was given in Section~\ref{lognormal}.

Say you  want $x$ to have cumulative distribution function $F(x)$. 
It is well known that you achieve this by taking $x=F^{-1}(\Phi(u))$,
where $\Phi$ is the where $\Phi$ is the cumulative distribution function 
of the $N(0,1)$ distribution. For a few common distributions, the composite
transformation $F^{-1}(\Phi(u))$ has been coded up for you in ADMB-RE,
and all you have to do is:
\begin{enumerate}
  \item Define a random effect $u$ with a $N(0,1)$ distribution.
  \item Transform $u$ into a new random effect $x$ using one of
        \texttt{something\_deviate} functions described below.
\end{enumerate}
where \texttt{something} is the name of the distribution.

As an example, say we want to obtain av vector \texttt{x} of gamma distributed 
random effects (probability density $x^{a-1}\exp(-x)/\Gamma(a)$). We can 
then use the code:
\begin{lstlisting}

PARAMETER_SECTION
  init_number a                                // Shape parameter
  init_number lambda                           // Scale parameter
  vector x(1,n)
  random_effects_vector u(1,n)
  objective_function_value g

PROCEDURE_SECTION
  g -= -0.5*norm2(u);          	               // N(0,1) likelihood contr.
  for (i=1;i<=n;i++) 
    x(i) = lambda*gamma_deviate(u(i),a);

\end{lstlisting}
\paragraph{Full example:} http://www.otter-rsch.com/admbre/examples/gamma/gamma.html

Similarly, to obtain beta$(a,b)$ distributed random effects, 
with density $f(x)\propto x^{a-1}(1-x)^{b-1}$, we use:
\begin{lstlisting}

PARAMETER_SECTION
  init_number a
  init_number b

PROCEDURE_SECTION
  g -= -0.5*norm2(u);                           // N(0,1) likelihood contr.
  for (i=1;i<=n;i++) 
    x(i) = beta_deviate(u(i),a,b);

\end{lstlisting}
The function \texttt{beta\_deviate()} has a fourth (optional) parameter that controls
the accuracy of the calculations. To learn more about this you will have
to dig into the source code. You find the code for \texttt{beta\_deviate()}
in the file \texttt{df1b2betdev.cpp}. The mechanism for specifying
default parameter values are found in the source file \texttt{df1b2fun.h}.

A third example is provided by the ``robust" normal distribution with probability density
$$
  f(x) = 0.95\frac{1}{\sqrt{2\pi}}e^{-0.5x^2}
         + 0.05\frac{1}{c\sqrt{2\pi}}e^{-0.5(x/c)^2}
$$
where $c$ is a ``robustness" parameter which by default is set to $c=3$ in \texttt{df1b2fun.h}.
Note that this is a mixture distribution consisting of 95\% $N(0,1)$ and 5\% $N(0,c^2)$.
The corresponding ADMB-RE code is
\begin{lstlisting}
PARAMETER_SECTION
  init_number sigma                           // Standard deviations (almost)
  number c

PROCEDURE_SECTION
  g -= - 0.5*norm2(u);    // N(0,1) likelihood contribution from u's
  for (i=1;i<=n;i++) 
  {
    x(i) = sigma*robust_normal_mixture_deviate(u(i),c);        
  }

\end{lstlisting}

\paragraph{Can a, and c be estimate?}
As indicated by the data types used above:
\begin{itemize}
\item[$\bigstar$] 
    $a$ and $b$ are among the parameters that are being estimated.
\item[$\bigstar$] 
    $c$ cannot be estimated.
\end{itemize}
It would however be possible to write a version of \texttt{robust\_normal\_mixture\_deviate}
where also $c$ and the mixing proportion (fixed at $0.95$ here) can be estimated.
For this you need to look into the file \texttt{df1b2norlogmix.cpp}.
The list of distribution that can be used is likely to be expanded in the future.

\section{Built in data likelihoods}
In simple \texttt{simple.tpl} the mathematical expressions for all log-likehood contributions
where written out in full detail. You may have hoped that for the most common
probability distributions there were functions written so that
you do not have to remember or look up their log-likelihood expressions. In case your
density are among those given in Table~\ref{tab:distributions} you are
lucky. More functions are likely to be implemented over time,
and user contributions are welcomed!

We stress that these functions should be only be used for data likelihoods,
and in fact, they will not compile if you try to let $X$ be a random effect.
So for instance, if you have observations $x_i$ that are Poisson
distributed with expectation $\mu_i$ you would write
\begin{lstlisting}

    for (i=1;i<=n;i++)
      f -= og\_density\_poisson(x(i),mu(i));

\end{lstlisting}
Note that functions do not accept vector arguments.


\begin{table}
\begin{tabular}{cccc}
\hline 
Density & Expression & Parameters & Name\tabularnewline
\hline
Poisson & $\frac{\mu^{x}}{\Gamma(x+1)}e^{-\mu}$  & $\mu>0$ & \texttt{log\_density\_poisson}\\
Neg. binomial$^{1}$ & $\mu=E(X)$, $\tau=\frac{Var(X)}{E(X)}$ & $\mu,\tau>0$ & \texttt{log\_negbinomial\_density}\\
\hline
\end{tabular}\caption{Distributions which currently can be
used as high-level data distributions (for data $X$) in ADMB-RE. $^{1}$The expression for the
negative binomial distribution is omitted due to its somewhat complicated
form. Instead the parameterization, via the via the overdispersion
coefficient, is given. The interested reader can look at the actual
implementation in the source file \texttt{df1b2negb.cpp}
\label{tab:distributions}}

\end{table}


\section{Phases}
\index{phases}A very useful feature of ADMB is that it allows the model to be fit in different phases. In the
first phase you estimate only a subset of the parameters, with the remaining parameters being fixed at their
initial values. In the second phase more parameters are turned on, and so it goes. The phase in which a parameter
becomes active is specified in the declaration of the parameter. By default a parameter has phase 1. A simple
example would be
\begin{lstlisting}
    PARAMETER_SECTION
      init_number a(1)
      random_effects_vector b(1,10,2)
\end{lstlisting}
where \texttt{a} becomes active in phase 1, while \texttt{b}\ is a vector of length 10 that becomes active in phase 2. 
With random effects we have the following rule-of-thumb (which may not always apply):
\begin{itemize}
\item[Ph1] Activate all parameters in the data likelihood, except those related to random effects.
\item[Ph2] Activate random effects and theirs standard deviations.
\item[Ph3] Activate correlation parameters (of random effects)
\end{itemize}
In complicated models it may be useful to break Ph1 into several sub-phases. 

During program development it is often useful to be able to completely switch a parameters off. A parameter
is inactivated when given phase `-1' as in
\begin{lstlisting}
    PARAMETER_SECTION
      init_number c(-1)
\end{lstlisting}
The parameter is still part of the program, and
its value will still be read from the pin-file, but it does not take part in the optimization (in any phase).

For further details about phases, please consult the section
`Carrying out the minimization in a number of phases' in the ADMB manual~\cite{admb_manual}.


\section{Penalized likelihood and empirical Bayes\label{sec:hood}}
The main question we answer in this section is: how are the random effects estimated, i.e. how
are the values that enters the \texttt{.par} and  \texttt{.std} calculated? Along the way
we will learn a little about how ADMB-RE works internally.

By now you should be familiar with the statistical interpretation of the random effects, 
but how are they treated internally in ADMB-RE? Since the random effects are not observed data they have parameter status, 
but we distinguish them from the hyper-parameters. In the marginal likelihood function used
internally by ADMB-RE to estimate hyper-parameters, the random effects are `integrated out'. The purpose of the
integration is to generate the marginal probability distribution for the observed quantities, which are $X$ and
$Y$ in \texttt{simple.tpl}. In that example we could have found an analytical expression for the marginal
distribution of $(X,Y)$, because only normal distributions were involved. For other distributions, such as the
binomial, no simple expression for the marginal distribution exists, and hence we must rely on ADMB to do the
integration. In fact, the core of what ADMB-RE does for you is to automatically calculates the marginal
likelihood, in its effort to estimate the hyper-parameters. 
\index{random effects!Laplace approximation}

The integration technique used by ADMB-RE is the so-called Laplace approximation~\cite{skaug_fournier1996aam}.
Somewhat simplified, the algorithm involves iterating between the following two steps:

\begin{enumerate}
\item The `penalized likelihood' step: Maximizing the likelihood with
respect to the random effects, while holding the value of the hyper-parameters fixed.
In \texttt{simple.tpl} this means doing the maximization w.r.t.~\texttt{x} only.
\item Updating the value of the hyper-parameters, using the estimates of the
random effects obtained in 1). 
\end{enumerate}
The reason for calling the objective function in 1) a penalized likelihood,
is that the prior on the random effects acts as a penalty function.

We can now return to the role of the initial values specified for the random effects in the \texttt{.pin} file.
Each time step 1) above is performed these values are used, unless you use the command line option \texttt{-noinit},
in which case the previous optimum is used as the starting value.

\paragraph{Empirical Bayes} is commonly used to refer to Bayesian estimates of the random effects, with
the hyper-parameters fixed at their maximum likelihood estimates. ADMB-RE uses
maximum aposteriori Bayesian estimates, as evaluated in step 1) above. 
Posterior expectation is a more commonly used as Bayesian estimator, but it requires additional calculations, and
is currently not implemented in ADMB-RE. For more details, see~\citeasnoun{skaug_fournier1996aam}.

The classical criticism of empirical Bayes is that the uncertain about the hyper-parameters is ignored,
and hence that the total uncertainty about the random effects is underestimated. ADMB-RE does however
take this into account and uses the following formula
\begin{equation}
\hbox{cov}(u)
  =
    -\left[\frac{\partial^{2}\log p(u|\hbox{data};\theta)}
  {\partial u\partial u^{\prime}}\right]^{-1}+ \frac{
  \partial u}{\partial\theta}
  \hbox{cov}(\theta) \left(\frac{\partial u}{\partial\theta}\right)^{\prime},
  \label{eq:EB_variance}
\end{equation}
where $u$ is the vector of random effect, $\theta$ is the vector of hyper-parameters, 
and $\partial u/\partial\theta$ is the sensitivity of the penelized likelihood estimator on the
value of $\theta$. The first term on the r.h.s.~is the ordinary Fisher information based variance
of $u$, while the second term accounts for the uncertainty in $\theta$.

\section{Building a random effects model that works}

In all nonlinear parameter estimation problems, there are two possible
explanations when your program does not produce meaningful results:

\begin{enumerate}
\item The underlying mathematical model is not well defined, e.g.~it may be
over-parameterized.
\item You have implemented the model incorrectly, e.g.~you have forgotten a
minus sign somewhere.
\end{enumerate}
In an early phase of the code development it may not be clear which of these
is causing the problem. With random effects, the two-step iteration scheme
described above makes it even more difficult to find the error. We therefore
advise you always to check the program on simulated data before you apply it to your real dataset. This
section gives you a recipe for how to do this.

\index{penalized likelihood}The first thing you should do after having finished the tpl-file is to check that
the penalized likelihood step is working correctly. In ADMB it is very easy to switch from a random effects
version of the program to a penalized likelihood version. In \texttt{simple.tpl} we would simply redefine the
random effects vector \texttt{x} to be of type \texttt{init\_vector}. The parameters would then be $a$, $b$,
$\mu $, $\sigma $, $\sigma_{x}$ and $x_{1},\ldots ,x_{n}$. It is not recommended, or even possible, to estimate
all of these simultaneously, so you should fix $\sigma_{x}$ (by giving it a phase `-1') at some reasonable
value. The actual value at which you fix $\sigma_{x}$ is not critically important, and you could even try a
range of $\sigma_{x}$ values. In larger models there will be more than one parameter that needs to be fixed.
We recommend the following scheme:
\begin{enumerate}
\item Write a simulation program (in R, S-Plus, Matlab, or some other
program) that generates data from the random effects model (using some
reasonable values for the parameters) and writes to \texttt{simple.dat}.

\item Fit the penalized likelihood program with $\sigma_{x}$ (or the
equivalent parameters) fixed at the value used to simulate data.

\item Compare the estimated parameters with the parameter values used to
simulate data. In particular, you should plot the estimated $x_{1},\ldots,x_{n}$
against the simulated random effects. The plotted points should center
around a straight line. If they do (to some degree of approximation) you
most likely have got a correct formulation of the penalized likelihood.
\end{enumerate}
If your program passes this test, you are ready to test the random effects version of the program. You redefine
\texttt{x} to be of type \texttt{random\_effects\_vector}, free up $\sigma_{x}$, and apply again your program to
the same simulated dataset. If the program produces meaningful estimates of the hyper-parameters, you most
likely have implemented your model correctly, and you are ready to move on to your real data!

With random effects it often happens that the maximum likelihood estimate of
a variance component is zero ($\sigma_{x}=0$). Parameters bouncing
against the boundaries usually makes one feel uncomfortable, but with random
effects the interpretation of $\sigma_{x}=0$ is clear and unproblematic. All
it really means is that data do not support a random effect, and the natural
consequence is to remove (or inactivate) $x_{1},\ldots ,x_{n}$, together
with the corresponding prior (and hence $\sigma_{x}$), from the model.


\section{MCMC}
\index{mcmc}
There are two different MCMC methods built into ADMB-RE: \texttt{-mcmc} and \texttt{-mcmc2}. Both are based on the
Metropolis-Hastings algorithm. The former generates a Markov chain
on the hyper-parameters only, while \texttt{-mcmc2} generates a chain on the joint vector of
hyper-parameters and random effects. (Some sort of rejection sampling could be used with \texttt{-mcmc} to generate values
also for the random effects, but this is currently not implemented). The advantages of~\texttt{-mcmc} are:
\begin{itemize}
\item Because there typically is a small number of hyper-parameters, but a large number of random effects, it is much
      easier to judge convergence of the chain generated by~\texttt{-mcmc} than that generated by~\texttt{-mcmc2}.
\item The \texttt{-mcmc}~chain mixes faster than the \texttt{-mcmc2}~chain.
\end{itemize}
The disadvantage of the \texttt{-mcmc} option is that it is slow, because it relies on evaluation of the
marginal likelihood by the Laplace approximation. It is recommended to run (separately) both of \texttt{-mcmc} and~\texttt{-mcmc2}
to verify that they yield the same posterior for the hyper-parameters.

\section{Importance sampling}
The Laplace approximation may be inaccurate in some situations \index{importance sampling}. 
The accuracy may be improved by adding an importance sampling step. 
This is done in ADMB-RE by using the command line argument \texttt{-is N seed}, where \texttt{N} 
is the sample size in the importance sampling and \texttt{seed} (optional) is used to 
initialize the random number generator. Increasing \texttt{N} will give better accuracy, at the cost 
of a longer run time. As a rule-of-thumb you should start with \texttt{N=100}, 
and increase \texttt{N} stepwise by a factor of 2 until the parameter estimates stabilize.

By running the model with different seeds you can check the Monte Carlo error in your estimates, and possibly
average across the different runs to decrease the Monte Carlo error.
Replaing the \texttt{-is N seed} option with a \texttt{-isb N seed} gives you a ``balanced"
sample, which in general should reduce the Monte Carlo error.

For large values of \texttt{N}, the option \texttt{-is N seed} will require a lot of memory,
and you will see that huge temporary files are produced during the execution of the program.
The option \texttt{-isf 5} will split the calculations relating to importance sampling
into 5 (or any number you like) batches. In combination
with the techniques discussed in Section~\ref{Memory_management}, this should reduce the 
storage requirements. An example of a command line is:
\begin{lstlisting}

  lessafre -isb 1000 9811 -isf 20 -cbs 50000000 -gdb 50000000

\end{lstlisting}

The \texttt{-is} option can also be used as a diagnostic tool for checking the accuracy of 
the Laplace approximation. If you add the \texttt{-isdiag} (print importance sampling) the 
importance sampling weights will be printed at the end of the optimization process. 
If these weights do not vary much, the Laplace approximation is probably doing well. 
On the other hand, if a single weight dominates the others by several orders of magnitude, 
you are in trouble, and it is likely that even \texttt{-is N} with a large value of \texttt{N} 
is not going to help you out. In such situations, reformulating the model, with the aim of making 
the loglikelihood closer to a quadratic function in the random effects, is the way to go. 
See also the following section.

\section{REML (Restricted maximum likelihood)}
\label{sec:reml}\index{reml}
It is well known that maximum likelihood estimators of variance parameters can be downwards biased. The biases arises from
estimation of one or more mean-related parameters. The simplest example of a REML estimator is the ordinary sample
variance
$$
s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar x)^2,
$$
where the divisor $(n-1)$, rather the $n$ which occurs for the maximum likelihood estimator, accounts for the fact that we 
have estimated a single mean parameter.

There are many ways of deriving the REML correction, but in the current context the most natural explanation is that
we integrate the likelihood function (note: not the log-likelihood) with respect to the mean parameters $\beta$, say. This is
achieved in ADMB-RE by defining $\beta$ as being of type \texttt{random\_effects\_vector}, but without
specifying a distribution/prior for the parameters. It should be noted that the only thing that
the \texttt{random\_effects\_vector} statement tells ADMB-RE is that the likelihood function should be integrated with respect
to $\beta$. In linear-Gaussian models the Laplace approximation is exact, and hence this approach yields exact
REML estimates. In nonlinear models the notion of REML is more difficult, but REML-like corrections are still being used.
For linear-Gaussian models the REML likelihood is available in closed form. Also many linear models can
be fitted with standard software packages. It is typically much simpler
to formulate a hierarchical model with explicit latent variables. 
As mentioned, the Laplace approximation is exact for Gaussian models, so it does not matter
what way you do it.


An example of such a model is found at
http://otter-rsch.com/admbre/examples/bcb/bcb.html.
To make the executable program run efficiently, the command line
options \texttt{-nr 1 -sparse}  should be used for linear models.
Also, note that REML estimates can be obtained as explained in section~\ref{sec:reml}.


\section{Improving performance~\label{sec:improving}}

In this section we discuss certain mechanisms you can use to make an ADMB-RE
program run faster and more smoothly.

\subsection{Reducing the size of temporary files}
\label{Memory_management}
\index{temporary files!reducing the size}When ADMB needs more temporary storage than is available in the
allocated memory buffers, it starts producing temporary files. Since writing to disk is much slower than
accessing memory, it is important to reduce the size of temporary files as much as possible. There are several
parameters (such as \texttt{arrmblsize}) built into ADMB that regulates how large memory buffers an ADMB program
allocates at startup. With random effects the memory requirements increase dramatically, and ADMB-RE deals with
this by producing (when needed) six temporary files:\index{temporary files!f1b2list1}
\begin{center}
\begin{tabular}{ll}
\textbf{File name} & \textbf{Command line option} \\ \hline
\texttt{f1b2list1} & \texttt{-l1 N} \\
\texttt{f1b2list12} & \texttt{-l2 N} \\
\texttt{f1b2list13} & \texttt{-l3 N} \\
\texttt{nf1b2list1} & \texttt{-nl1 N} \\
\texttt{nf1b2list12} & \texttt{-nl2 N} \\
\texttt{nf1b2list13} & \texttt{-nl3 N} \\ \hline
\end{tabular}%
\end{center}
The table also shows the command line arguments you can use to manually set the size (determined by \texttt{N}) of the different
memory buffers.

When you see any of these files start growing, you should kill your application and restart
it with the appropriate command line options. In addition to the options shown above
there is \texttt{-ndb N} that splits the computations into N chunks. This effectively reduces
the memory requirements by a factor of $N$, at the cost of a somewhat longer run time.
It is necessary that $N$ is a divisor of the total number of random effects in the model,
so that it is possible to split the job into $N$ equally large parts.
The \texttt{-ndb} option can be used in combination with the \texttt{-l} and \texttt{-nl} options listed above.
The following rule-of-thumb for setting $N$ in~\texttt{-ndb N} can be used: if there are totally $m$ random effects in the model,
one should choose $N$ such that $m/N\approx 50$.
For most of the models in the example collection (Chapter~3) this choice of $N$ prevents any temporary files of being created.

Consider the model \texttt{http://otter-rsch.com/admbre/examples/union/union.html} as an example. This model contains only about 60 random effects,
but does rather heavy computations with these, and as a consequence large temporary files are generated.
The following command line
\begin{lstlisting}
    $ ./union -l1 10000000 -l2 100000000 -l3 10000000 -nl1 10000000
\end{lstlisting}
takes away the temporary files but requires 80Mb of memory. The command line
\begin{lstlisting}
    $ ./union -est -ndb 5 -l1 10000000
\end{lstlisting}
also runs without temporary files, requires only 20Mb of memory, but runs three times slower.

Finally, a warning about the use of these command line options. If you allocate too much memory your application will crash,
and you will (should) get a meaningful error message. You should monitor the memory use of your application using ``Task
Manager" under Windows and the command ``top" under Linux, to ensure that you do not exceed the available memory
on your computer.

\subsection{Limited memory Newton optimization}
\index{limited memory quasi-Newton}The penalized likelihood step (Section \ref{sec:hood}), that forms a crucial
part of the algorithm used by ADMB to estimate hyper-parameters, is by default conducted using a quasi-Newton
optimization algorithm. If the number of random effects is large, as it typically is for separable models, it
may be more efficient to use a `limited memory quasi-Newton' optimization algorithm. This is done using the
command line argument \texttt{-ilmn N}, where \texttt{N} is the number of steps to keep. Typically \texttt{N=5}
is a good choice. 



\chapter{Exploiting separability}
\label{separability}
The following model classes:
\begin{itemize}
  \item Grouped or nested random effects
  \item State-space models
  \item Crossed random effects
   \item Latent Markov random fields
\end{itemize}
share an important property: their ``Hessian" is a sparse matrix. This enables
ADMB-RE to do the calculations very efficiently. The Hessian $H$ is defined
as the (negative) Fisher information matrix (inverse covariance matrix) of the posterior distribution of the random effects:
\begin{equation}
  p(u|y) \propto p(y|u)p(u),
  \label{p(u|y)}
\end{equation}
where $u$ are the random effect and $y$ are the data. This definition is only excact if both $u$ and $y$ are
Gaussian. More generally, $H$ is the Hessian matrix of the function $\log[p(\cdot|y)]$.

That $H$ is sparse means that it contains mostly zeros. The actual sparsity pattern depends on the model type:
\begin{itemize}
  \item Grouped or nested random effects: $H$ is block diagonal.
  \item State-space models: $H$ is a banded matrix with a narrow band.
  \item Crossed random effects: unstructured sparsity pattern.
   \item Latent Markov random fields: often banded, but with a wide band.
\end{itemize}
ADMB-RE should print out a message such as
\begin{lstlisting}
  Block diagonal Hessian (Block size = 3)
\end{lstlisting}
at the beginning of the phase when the random effects are becoming active parameters. 

Not all models are separable, and for small toy examples (less than 50 random effects, say)
we do not need to care about separability. But chances are high that you need to become
familiar with the concept. How do we inform ADMB-RE that the model is separable?
We shall see the key is \texttt{SEPARABLE\_FUNCTION}'s,  which are invoked
many times with a small number of the random effects as arguments each time.
Do you think that ADMB-RE should be able to detect the separable structure
on its own? Well, maybe so, but after you have worked with a few separable
models you will find that having to call the \texttt{SEPARABLE\_FUNCTION}'s
actually structures your own way of thinking about the model. This is especially
so for state-space models. A good reference (although a bit advanced) on conditional probabilities is~\citeasnoun{rue2005gaussian}.


\section{The first example}
A simple example is the one-way variance component model
\[
  y_{ij}=\mu +\sigma_u u_{i}+\varepsilon_{ij}, \qquad i=1,\ldots ,q,\quad j=1,\ldots ,n_{i}
\]
where $u_{i}\sim N(0,1)$ is a random effect and $\varepsilon_{ij}\sim N(0,\sigma^2)$ 
is an error term. The straightforward implementation of this model (shown only in part) is
\begin{lstlisting}

PARAMETER_SECTION
  random_effects_vector u(1,q)

PROCEDURE_SECTION
  for(i=1;i<=q;i++)
  {
    g -= -0.5*square(u(i));
    for(j=1;j<=n(i);j++)
      g -= -log(sigma) - 0.5*square((y(i,j)-mu-sigma_u*u(i))/sigma);
  }

\end{lstlisting}
The efficient implementation of this model is 
\begin{lstlisting}

PROCEDURE_SECTION
  for(i=1;i<=q;i++)
    g_cluster(i,u(i),mu,sigma,sigma_u);

SEPARABLE_FUNCTION void g_cluster(int i, const dvariable& u,...)
  g -= -0.5*square(u);
  for(int j=1;j<=n(i);j++)
    g -= -log(sigma) - 0.5*square((y(i,j)-mu-sigma_u*u)/sigma);

\end{lstlisting}
where \texttt{...} replaces the rest of the argument list (due to lack of space in this document).

It is the function call \texttt{g\_cluster(i,u(i),mu,sigma,sigma\_u)} that enables ADMB-RE to 
identify that the posterior distribution~(\ref{p(u|y)}) factors (over $i$):
\[  
  p(u|y) \propto \prod_{i=1}^{q}\left\{ \prod_{j=1}^{n_{i}}p(u_{i}|y_{ij})\right\} 
\]
and hence that the Hessian is  block diagonal (with block size 1).
Knowing that the Hessian is  block diagonal enables ADMB-RE to do a series of univariate
Laplace approximations, rather than a single Laplace approximation in full dimension~$q$. It should
then be possible to fit models where $q$ is in the order of thousands, but this clearly
depends on the complexity of the function \texttt{g\_cluster}.

The following rules apply:
\begin{itemize}
\item[$\bigstar$] The argument list in the definition of the \texttt{SEPARABLE\_FUNCTION} \underline{should not} broken
	          into several lines of text in the tpl-file. This is often tempting as the line typically gets long,
  		  but it results in an error message from \texttt{tpl2rem}.
\item[$\bigstar$] Objects defined in the \texttt{PARAMETER\_SECTION} \underline{must} be 
		  passed as arguments to \texttt{g\_cluster}. There is one exception: 
		  the objective function \texttt{g} is a global object, and does not need to be an argument.
                  Temporary/programming variables should be defined locally within the \texttt{SEPARABLE\_FUNCTION}.
\item[$\bigstar$] Objects defined in the \texttt{DATA\_SECTION} \underline{should not} be passed as arguments
		  to \texttt{g\_cluster} (they are also global objects).
\end{itemize}
The data types that currently can be passed as arguments to a \texttt{SEPARABLE\_FUNCTION} are:
\begin{lstlisting}

  int
  const dvariable&
  const dvar_vector&
  const dvar_matrix&

\end{lstlisting}
with an example being
\begin{lstlisting}
SEPARABLE_FUNCTION void f(int i, const dvariable& a, const dvar_vector& beta)
\end{lstlisting}
The qualifier \texttt{const} is required for the latter two data types, and signalizes to the 
C++~compiler that the value of the variable is not going to be changed by the function.
You may also come across the type \texttt{const prevariable\&} which means exactly the same as 
\texttt{const dvariable\&}.

There are other rules that have to be obeyed:
\begin{itemize}
  \item[$\bigstar$] No calculations involving variables defined in the \texttt{PARAMETER\_SECTION} 
		  are allowed in the \texttt{PROCEDURE\_SECTION}. The only use of such variables there 
			is passing them as arguments to \texttt{SEPARABLE\_FUNCTION}'s.
\end{itemize}
This rule implies that all the action has to take place inside the \texttt{SEPARABLE\_FUNCTION}'s.
To minimize the number of parameters that have be passed as arguments, the following programming
practice is recommended when using \texttt{SEPARABLE\_FUNCTION}'s:
\begin{itemize}
  \item[$\bigstar$] The \texttt{PARAMETER\_SECTION} should contain definitions only of 
		    the independent parameters (those variables which type has 
		    a \texttt{init\_} prefix) and random effects, i.e.~no temporary 
		    programming variables.
\end{itemize}
All temporary variables needed for the computations should be defined locally in
the \texttt{SEPARABLE\_FUNCTION} as shown here:
\begin{lstlisting}	

SEPARABLE_FUNCTION void prior(const dvariable& log_s, const dvariable& u)
  dvariable sigma_u = exp(log_s);
  g -= -log_s - 0.5*square(u(i)/sigma_u);

\end{lstlisting}

\paragraph{Full example} http://otter-rsch.com/admbre/examples/orange/orange.html.
The orange model has block size 1, 


\section{Nested or clustered random effects: Block diagonal $H$}
\label{sec:nested}
In the above model there was no hierarchical structure among the 
latent random variables (the \texttt{u}'s). A more complicated example
is provided by the following model:
\[
  y_{ijk}= \sigma_v v_{i} + \sigma_u u_{ij}+\varepsilon_{ijk},
            \qquad i=1,\ldots ,q,\quad j=1,\ldots ,m,\quad k=1,\ldots ,n_{ij},
\]
where the random effects $v_{i}$ and $u_{ij}$ are independent $N(0,1)$ distributed,
and $\varepsilon_{ijk}\sim N(0,\sigma^2)$ is still the error term. 
One often says that the \texttt{u}'s are nested within the \texttt{v}'s.

Another perspective is that the data can be split into independent clusters.
For $i_1\neq i_2$ we have that $y_{i_1jk}$ and $y_{i_2jk}$ are statistically independent,
so that the likelihood factors at the outer nesting level ($i$). 

To exploit this we use the \texttt{SEPARABLE\_FUNCTION} as follows:
\begin{lstlisting}	

PARAMETER_SECTION
  random_effects_vector v(1,q)
  random_effects_matrix u(1,q,1,m)

PROCEDURE_SECTION
  for(i=1;i<=q;i++)
    g_cluster(v(i),u(i),sigma,sigma_u,sigma_v,i);

\end{lstlisting}
Note that \texttt{u(i)} is the i'th row of the matrix \texttt{u} (this is standard ADMB stuff),
and it should be passed as a vector to the \texttt{SEPARABLE\_FUNCTION}, which
we would implement as follows:
\begin{lstlisting}	

SEPARABLE_FUNCTION void g_cluster(const dvariable& v,const dvar_vector& u,...)
  g -= -0.5*square(v);
  g -= -0.5*norm2(u);

  for(int j=1;j<=m;j++)
    for(int k=1;k<=n(i,j);k++)
      g -= -log(sigma) - 0.5*square((y(i,j,k)
                  -sigma_v*v - sigma_u*u(j))/sigma);

\end{lstlisting}
\begin{itemize}
\item[$\bigstar$] For a model to be detected as ``Block diagonal Hessian" each latent variable
          should be passed \underline{exactly once} as an argument to a \texttt{SEPARABLE\_FUNCTION}.
\end{itemize}
To ensure that you have not broken this rule you should look for an message like this at run time:
\begin{lstlisting}
  Block diagonal Hessian (Block size = 3)
\end{lstlisting}
It is possible that the groups or clusters (as indexed by $i$ in this case) are of different size.
Then the ``Block diagonal Hessian" that is printed is an average.


Alternative, we could have structured the program as follows:
\begin{lstlisting}	

PARAMETER_SECTION
  random_effects_vector v(1,q)
  random_effects_matrix u(1,q,1,m)

PROCEDURE_SECTION
  for(i=1;i<=q;i++)
    for(j=1;j<=m;j++)
      g_cluster(v(i),u(i,j),sigma,sigma_u,sigma_u,i);

\end{lstlisting}
but this would not be detected by ADMB-RE as a clustered model (because
\texttt{v(i)} is passed multiple times), and hence
ADMB-RE will not be able to take advantage of the fact that the likelihood
factors. However, the use of the \texttt{SEPARABLE\_FUNCTION} makes
it possible for ADMB-RE to perform efficient calculations when invoked
with the \texttt{-shess} \index{sparse matrix methods} command line option as described later.

\subsection{Gauss-Hermite quadrature}
\index{Gauss-Hermite quadrature}
In the situation where the model is separable of type "block diagonal Hessian" with only a single
random effect in each block (see Section~\ref{separability}), Gauss-Hermite quadrature
is available as an option to the Laplace approximation and the \texttt{-is} option (importance sampling).
It is invoked with command line option \texttt{-gh N} where \texttt{N} is the number of quadrature points.

\subsection{Frequency weighting for multinomial likelihoods}
In situations were the response variable only can take on a finite
number of different values, it is possibly to reduce the computational burden
enormously. As an example, consider a situation where observation $y_{i}$ 
is binomially distributed with parameters $N=2$ and $p_{i}$. Assume that
$$
  p_{i}=\frac{\exp (\mu +u_{i})}{1+\exp (\mu +u_{i})},
$$
where $\mu $ is a parameter and $u_{i}\sim N(0,\sigma ^{2})$ is a random
effect. For independent observations $y_1,\ldots,y_n$, 
the loglikelihood function for the parameter 
$\theta =(\mu ,\sigma )$ can be written:
\begin{equation}
  l(\theta )=\sum_{i=1}^{n}\log \left[ p(x_{i};\theta )\right] .
\end{equation}
In ADMB-RE $p(x_{i};\theta)$ is approximated using the Laplace approximation. 
However, since $y_i$ only can take the values $0$, $1$ and $2$, we can 
re-write the loglikelihood as 
$$
l(\theta )=\sum_{j=0}^{2}n_{j}\log \left[ p(j;\theta )\right] ,  
$$
where $n_j$ is the number $y_i$'s being equal to $j$. Still the Laplace
approximation must be used to approximate $p(j;\theta )$, but now only for $%
j=0,1,2$, as opposed to $n$ times above. For large $n$ this can give large
a large reduction in computing time.

To implement the weighted loglikelihood~(\ref{l_w}) we define a weight 
vector $(w_1,w_2,w_3)=(n_{0},n_{1},n_{2})$. To read the weights from file, 
and to tell ADMB-RE that \texttt{w} is a weights vector, the following code is used:
\begin{lstlisting}

DATA_SECTION
 init_vector w(1,3)		

PARAMETER_SECTION
 !! set_multinomial_weights(w);

\end{lstlisting}
In addition it is necessary to explicitly multiply the likelihood contributions
in~(\ref{l_w}) by $w$. The program must be written with \texttt{SEPARABLE\_FUNCTION} as 
explained in Section~\ref{sec:nested}. For the likelihood~(\ref{l_w})
the \texttt{SEPARABLE\_FUNCTION} will be invoked three times.

\paragraph{Full example:} http://www.otter-rsch.com/admbre/examples/weights/weights.html


\section{State-space models: Banded $H$}
\label{sec:state-space}\index{state-space models}
A simple state space model is
\begin{eqnarray*}
  y_i &=& u_i + \epsilon_i,\\
  u_i &=& \rho u_{i-1} + e_i,
\end{eqnarray*}
where $e_i\sim N(0,\sigma^2)$ is an innovation term. The log-likelihood contribution
coming from the state vector $(u_1,\ldots,u_n)$ is 
\[
  \sum_{i=2}^n \log\left(\frac{1}{\sqrt{2\pi }\sigma }
                \exp\left[-\frac{(u_{i}-\rho u_{i-1})^{2}}{2\sigma^2}\right]\right),
\]
where $(u_1,\ldots,u_n)$ is the state vector. To make ADMB-RE exploit this 
special structure we write a \texttt{SEPARABLE\_FUNCTION} named
\texttt{g\_conditional}, that implements the individual terms in the above sum.
This function would then be invoked as follows 
\begin{lstlisting}

    for(i=2;i<=n;i++)
      g_conditional(u(i),u(i-1),rho,sigma);

\end{lstlisting}
\paragraph{Full example} http://www.otter-rsch.com/admbre/examples/polio/polio.html.

Above we have looked at a model with a univariate state vector. For multivariate
state vectors, as in 
\begin{eqnarray*}
  y_i &=& u_i + v_i +\epsilon_i,\\
  u_i &=& \rho_1 u_{i-1} + e_i,\\
  v_i &=& \rho_2 v_{i-1} + d_i,
\end{eqnarray*}
we would merge the $u$ and $v$ vectors into a single vector
$(u_1,v_1,u_2,v_2,\ldots,u_n,v_n)$, and define
\begin{lstlisting}

   random_effects_vector u(1,m)

\end{lstlisting}
where $m=2n$. The call to the \texttt{SEPARABLE\_FUNCTION} would now look like
\begin{lstlisting}

 for(i=2;i<=n;i++)
  g_conditional(u(2*(i-2)+1),u(2*(i-2)+2),u(2*(i-2)+3),u(2*(i-2)+4),...);

\end{lstlisting}
where \texttt{...} denotes the arguments $\rho_1$, $\rho_2$, $\sigma_e$ and $\sigma_d$.

\section{Crossed random effects: sparse $H$}
\index{crossed effects}
The simplest instance of a crossed random effects model is
\[
  y_{k}= \sigma_u u_{i(k)} + \sigma_v v_{j(k)}+\varepsilon_{k},
            \qquad i=1,\ldots n,
\]
where $u_{1},\ldots,u_{N}$ and $v_{1},\ldots,v_{M}$ are random effects, 
and where $i(k)\in\{1,N\}$  and $j(k)\in\{1,M\}$ are index maps. The 
$y$'s sharing either a $u$ or a $v$ will be dependent, and in general
no complete factoring of the likelihood will be possible. However, it is still
important to exploit the fact that the $u$'s and $v$'s only enter the 
likelihood through pairs $(u_{i(k)},v_{j(k)})$. Here is the code for the
crossed model:
\begin{lstlisting}

 for (k=1;k<=n;k++)
  log_lik(k,u(i(k)),v(j(k)),mu,s,s_u,s_v);

 SEPARABLE_FUNCTION void log_lik(int k, const dvariable& u,...)
  g -=  -log(s) - 0.5*square((y(k)-(mu + s_u*u + s_v*v))/s);

\end{lstlisting}
If only a small proportion of all the possible combinations of $u_i$ and $v_j$ 
actually occurs in the data, then the posterior covariance matrix of
$(u_{1},\ldots,u_{N},v_{1},\ldots,v_{M})$ will be sparse. 
When an executable program produced by ADMB-RE is invoked with the 
\texttt{-shess} command line option, sparse matrix calculations are used. 

This is useful not only for crossed models. Here are a few other applications:
\begin{itemize}
\item For the nested random effects model as explained in section~\ref{sec:nested}.
\item REML estimation; recall that REML estimates are obtained by making
      a fixed effect random, but with no prior distribution. 
      For the nested models in section~\ref{sec:nested},
      and the models with state-space structure of section~\ref{sec:state-space},
      when using REML, ADMB-RE will to detect the cluster or time series structure
      of the likelihood. (This has to do with the implementation of ADMB-RE, not
      the model itself). However, the posterior covariance will still be sparse,
      and the use of \texttt{-shess} is advantageous.\index{reml}
\end{itemize}



\section{Gaussian priors and quadratic penalties\label{gaussianprior}}
\index{prior distributions!Gaussian priors}In most models the
prior for the random effect will be Gaussian. In some situations,
such as in spatial statistics, all the individual components of the random effects vector will be jointly
correlated. ADMB contains a special feature (the \texttt{normal\_prior} keyword) for dealing efficiently with such
models. The construct used to declaring a correlated Gaussian prior is
\begin{lstlisting}
    random_effects_vector u(1,n)
    normal_prior S(u);
\end{lstlisting}
The first of these lines is an ordinary declaration of a random effects vector. The second line tells ADMB that
\texttt{u} has a multivariate Gaussian distribution with zero expectation and covariance matrix \texttt{S} ,
i.e. the probability density of $\mathbf{u}$ is
\[
h(\mathbf{u)=}\left( 2\pi \right) ^{-\dim(S)/2}\det (S)^{-1/2}\exp \left( - \frac{1}{2}\mathbf{u}^{\prime
}S^{-1}u\right) .
\]
Here, $S$ is allowed to depend on the hyper-parameters of the
model. The part of the code where \texttt{S} gets assigned its
value must be placed in a \texttt{SEPARABLE\_FUNCTION} (see

\begin{itemize}
\item[$\bigstar$] The log-prior $\log \left( h\left(
\mathbf{u}\right) \right) $ is automatically subtracted from the
objective function. It is thus necessary that the objective
function holds the negative loglikelihood when using the
\texttt{normal\_prior}.
\item[$\bigstar$] To verify that your
model really is partially separable you should try replacing the \texttt{SEPARABLE\_FUNCTION} keyword with an
ordinary \texttt{FUNCTION}. Then verify on a small subset of your data that the two versions of the program
produce the same results. You should be able to observe that the \texttt{SEPARABLE\_FUNCTION}-version runs
faster.
\end{itemize}
\paragraph{Full example}
\texttt{http://otter-rsch.com/admbre/examples/spatial/spatial.html}).


\appendix

\chapter{Example collection}
\label{sec:example_collection}
This section contains various examples of how to use ADMB-RE. Some of
these has been referred to earlier in the manual. The exampels are grouped according
to their ``Hessian type" (see Section~\ref{separability}). At the end of each example
you will find a {\bf Files} section containing links to webpages where both program code
and data can be downloaded.

\section{Non-separable models}
This section contains models which do not use any of the separability stuff.
Sections~\ref{sec:gam} and~\ref{sec:lidar}
illustrate how to use splines as non-parametric compoents. This is currenlty
a very popular technqiue, and fits very nicely
into the random effects framework~\cite{rupp:wand:carr:2003}. 
All the models, except the first, are in fact separable, but for illustrative purposes (the code becomes easier to read)
this has been ignored.

\newpage

\subsection{Mixed logistic regression; a WinBUGS comparison}
\label{sec:logistic_example}
Mixed regression regression models will usually have a block diagonal Hessian
du to grouping/clustering of the data. The present model was deliberately
chosen not to be separable, in order to pose a computational challenge
to both ADMB-RE and WinBUGS. 

\paragraph{Model description}
Let $\mathbf{y}=(y_1,\ldots,y_n)$ be a vector of dichotomous observations
($y_i\in\{0,1\}$), and let $\mathbf{u}=(u_1,\ldots,u_q)$ be a vector of
independent random effects, each with Gaussian distribution (expectation $0$ and
variance $\sigma^2$). Define the success probability $\pi_i=\Pr(y_i=1)$. The
following relationship between $\pi_i$ and explanatory variables (contained in
matrices $\mathbf{X}$ and $\mathbf{Z}$) is assumed:
\[
  \log\left(\frac{\pi_i}{1-\pi_i}\right) = \mathbf{X}_i\mathbf{\beta} +
  \mathbf{Z}_i\mathbf{u},
\]
where $\mathbf{X}_i$ and $\mathbf{Z}_i$ are the $i$'th rows of the known
covariates matrices $\mathbf{X}$ ($n\times p$) and $\mathbf{Z}$ ($n\times q$),
respectively, and $\mathbf{\beta}$ is a $p$-vector of regression parameters.
Thus, the vector of fixed-effects vector is
$\mathbf{\theta}=(\mathbf{\beta},\log\sigma)$.

\paragraph{Results}
The goal here is to compare computation times with BUGS on a simulated data set.
For this purpose we use $n=200$, $p=5$, $q=30$, and values of the the hyper
parameters as shown in the table below (`True values'). The matrices
$\mathbf{X}$ and $\mathbf{Z}$ were generated randomly with each element
uniformly distributed on $[-2,2]$. As start values for both AD Model Builder and
BUGS we used $\beta _{\mathrm{init},j}=-1$ and $\sigma_\mathrm{init}=4.5$. In
BUGS we used a uniform $[-10,10]$ prior on $\beta_j$ and a standard (in the BUGS
literature) noninformative gamma prior on $\tau=\sigma^{-2}$. In AD Model
Builder the parameter bounds $\beta_j\in[-10,10]$ and $\log\sigma\in[-5,3]$ were
used in the optimization process.
\begin{center}
  \begin{tabular}{lrrrrrr}
    \hline
    ~           & $\beta_1$ & $\beta_2$ & $\beta_3$ & $\beta_4$ & $\beta_5$ & $\sigma$\\
    \hline
    True values & 0.0000    &  0.0000   & 0.0000    & 0.0000    &  0.0000   & 0.1000  \\
    ADMB-RE     & 0.0300    & -0.0700   & 0.0800    & 0.0800    & -0.1100   & 0.1700  \\
    Std.\ dev.  & 0.1500    &  0.1500   & 0.1500    & 0.1400    &  0.1600   & 0.0500  \\
    WinBUGS     & 0.0390    & -0.0787   & 0.0773    & 0.0840    & -0.1041   & 0.1862  \\
    \hline
  \end{tabular}
\end{center}
On the simulated dataset AD Model Builder used $27$ seconds to converge to the
optimum of likelihood surface. On the same dataset we first ran WinBUGS (Version
1.4) for $5,000$ iterations. The recommended convergence diagnostic in WinBUGS
is the Gelman-Rubin plot (see the help files available from the menues in
WinBUGS) which require that two Markov chains are run in parallel. From the
Gelman-Rubin plot it was clear that convergence appeared after approximately
$2,000$ iterations. The time taken by WinBUGS to perform generate the first
$2,000$ was approximately $700$ seconds.

\paragraph{Files} http://otter-rsch.com/admbre/examples/logistic/logistic.html

\newpage

\subsection{Generalized additive models (GAM's)}
\label{sec:gam}

\paragraph{Model description}
\index{nonparametric estimation!splines}A very useful generalization of the
ordinary multiple regression 
\[
y_{i}=\mu +\beta _{1}x_{1,i}+\cdots +\beta _{p}x_{p,i}+\varepsilon _{i},
\]%
is the class of additive models, 
\begin{equation}
y_{i}=\mu +f_{1}(x_{1,i})+\cdots +f_{p}(x_{p,i})+\varepsilon _{i}.
\label{eqn:gam}
\end{equation}%
\index{GAM}Here, the $f_{j}$ are `nonparametric' components which can be
modelled by penalized splines. When this generalization is carried over to
generalized linear models, and we arrive at the class of GAM's \cite%
{hast:tibs:1990}. From a computational perspective penalized splines are
equivalent to random effects, and thus GAM's fall naturally into the domain
of ADMB-RE.

For each component $f_{j}$ in (\ref{eqn:gam}) we construct a design matrix $%
\mathbf{X}$ such that $f_{j}(x_{i,j})=\mathbf{X}^{(i)}\mathbf{u}$, where $%
\mathbf{X}^{(i)}$ is the $i$th row of $\mathbf{X}$ and $\mathbf{u}$\ is a
coeffisient vector. We use the R-function \texttt{splineDesign} (from the 
\texttt{splines} library) to construct a design matrix $\mathbf{X}$. To
avoid overfitting we add a first order difference penalty%
\index{splines!difference penalty} \cite{eile:marx:1996} :%
\begin{equation}
-\lambda ^{2}\sum_{k=2}\left( u_{k}-u_{k-1}\right) ^{2},
\label{eqn:first_order}
\end{equation}%
to the ordinary GLM loglikelihood, where $\lambda $ is a smoothing parameter
to be estimated. By viewing $\mathbf{u}$ as a random effects vector\ with
the above Gaussian prior, and by taking $\lambda $ as a hyper-parameter, it
becomes clear that GAM's are naturally handled in ADMB-RE.

\paragraph{Implementation details}
\begin{itemize}
\item A computationally more efficient implementation is obtained by moving $%
\lambda $ from the penalty term to the design matrix, i.e. $%
f_{j}(x_{i,j})=\lambda ^{-1}\mathbf{X}^{(i)}\mathbf{u}$.

\item Since (\ref{eqn:first_order}) does not penalize the mean of $\mathbf{u,%
}$ we impose the restriction that $\sum_{k=1}u_{k}=0$ (see the \texttt{%
union.tpl} for details). Without this restriction the model would be
over-parameterized since we allready have an overall mean $\mu $ in (\ref%
{eqn:gam}).

\item To speed up computations the parameter $\mu $ (and other regression
parameters) should be given `phase 1' in ADMB, while the $\lambda $'s and
the $\mathbf{u}$'s should be given given `phase 2'.
\end{itemize}

\begin{figure}[H]
\includegraphics[width=6in]{union_fig.pdf}
\caption{Probablity of membership as a function of covariates. In each plot,
the remaining covariates are fixed at their sample means. The effective
degrees of freedom (df) are also given \protect\cite{hast:tibs:1990}$.$}
\label{fig:union}
\end{figure}

\paragraph{The Wage-union data}
The data, which are available from Statlib (\texttt{lib.stat.cmu.edu/}),
contain information for each of 534 workers about whether they are members ($%
y_{i}=1$) of a workers union or not ($y_{i}=0$). We study the probability of
membership as a function of six covariates. Expressed in the notation used
by the R (S-Plus) function \texttt{gam} the model is:
\begin{verbatim}
   
    union ~race + sex + south + s(wage) + s(age) + s(ed), family=binomial
 
\end{verbatim}

Here, \texttt{s()} denotes a spline functions with 20 knots each. For 
\texttt{wage} a cubic spline is used, while for \texttt{age}\ and \texttt{ed}
quadratic splines are used. The total number of random effects that arrise
from the three corresponding $\mathbf{u}$ vectors is 64. Figure \ref%
{fig:union} shows the estimated nonparametric components of the model. The
time taken to fit the model was 165 seconds.


\paragraph{Extentions}
\begin{itemize}
\item The linear predictor may be a mix of ordinary regression terms ($%
f_{j}(x)=\beta _{j}x$) and nonparametric terms. ADMB-RE offers a unified
approach to fitting such models, in which the smoothing parameters $\lambda
_{j}$ and the regression parameters $\beta _{j}$ are estimated
simultaneously.

\item It is straight forward in ADMB-RE to add `ordinary' random effects to
the model, for instance to accomodate for correlation within groups of
observations, as in \citeasnoun{lin:zhan:1999}.
\end{itemize}

\paragraph{Files} http://otter-rsch.com/admbre/examples/union/union.html

\newpage

\subsection{Semi-parametric estimation of mean and variance}
\label{sec:lidar}
\index{nonparametric estimation!variance function}
\paragraph{Model description}
An assumption underlying the ordinary regression 
\[
y_{i}=a+bx_{i}+\varepsilon _{i}^{\prime }
\]%
is that all observations have the same variance, 
i.e. Var$\left( \varepsilon_{i}^{\prime }\right) =\sigma^{2}$. 
This assumption does not always hold,
as for the data shown in the upper panel of Figure \ref{fig:lidar}. 
This example is taken from~\citeasnoun{rupp:wand:carr:2003}.

It is clear that the variance increases to the right (for large values of $x$). It is also clear
that the mean of $y$ is not a linear function of $x$. We thus fit the model%
\[
y_{i}=f(x_{i})+\sigma (x_{i})\varepsilon _{i},
\]%
where $\varepsilon _{i}\sim N(0,1),$ and $f(x)$ and $\sigma (x)$ are
modelled nonparametrically. We take $f$ to be a
penalized spline. To ensure that $\sigma (x)>0$ we model $\log \left[ \sigma
(x)\right] ,$ rather than $\sigma (x)$, as a spline function. For $f$ we use
a cubic spline (20 knots) with a 2nd order difference penalty%
\[
-\lambda ^{2}\sum_{k=3}^{20}\left( u_{j}-2u_{j-1}+u_{j-2}\right) ^{2},
\]%
while we take $\log \left[ \sigma (x)\right] $ to be a linear spline (20
knots) with the 1st order difference penalty (\ref{eqn:first_order}).

\paragraph{Implementation details}
Details on how to implement spline components are given Example \ref{sec:gam}.

\begin{itemize}
\item Parameter associated with $f$ should be given `phase 1' in ADMB, while
those associated with $\sigma $ should be given `phase 2'. The reason is
that in order to estimate the variation, one first needs to have fitted the
mean part. 
\end{itemize}

\begin{itemize}
  \item In order to estimate the variation function, one first needs to have
  fitted the mean part. Parameter associated with $f$ should thus be given
  `phase 1' in ADMB, while those associated with $\sigma$ should be given `phase
  2'.
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[width=4in]{lidar_fig.pdf}
  \caption{LIDAR data (upper panel) used by Ruppert et al.\ (2003) with fitted
    mean. Fitted standard deviation is shown in the lower panel.}
  \label{fig:lidar}
\end{figure}

\paragraph{Files} http://otter-rsch.com/admbre/examples/lidar/lidar.html

\subsection{Weibull regression in survival analysis}
\paragraph{Model description}

\label{sec:kidney_example}
A typical setting in survival analysis is that we observe the time point $t$ at
which the death of a patient occurs. Patients may leave the study (for some
reason) before they die. In this case the survival time is said to be censored,
and $t$ refers to the time point when the patient left the study. The indicator
variable $\delta$ is used to indicate whether $t$ refers to the death of the
patient ($\delta=1$) or to a censoring event ($\delta=0$). The key quantity in
modelling the probability distribution of $t$ is the hazard function $h(t)$,
which measures the instantaneous death rate at time $t$. We also define the
cumulative hazard function $\Lambda(t)=\int_0^th(s)ds$, implicitly assuming that
the study started at time $t=0$. The log likelihood contribution from our
patient is $\delta\log(h(t))-H(t)$. A commonly used model for $h(t)$ is Cox's
proportional hazard model, in which the hazard rate for the $i$th patient is
assumed to be on the form
\[
  h_it = h_0(t)\exp(\eta_i\mathbf), \qquad i=1,\ldots n.
\]
Here, $h_0(t)$ is the ``baseline'' hazard function (common to all patients) and
$\eta_i=\mathbf{X}_i\mathbf{\beta}$, where $\mathbf{X}_i$ is a covariate vector
specific to the $i$th patient and $\mathbf{\beta}$ is a vector of regression
parameters. In this example we shall assume that the baseline hazard belongs to
the Weibull family: $h_0(t)=rt^{r-1}$ for $r>0$.

In the collection of examples following the distribution of WinBUGS this model
is used to analyse a dataset on times to kidney infection for a set of $n=38$
patients (Kidney:\ Weibull regression with random effects, Examples Volume 1,
WinBUGS 1.4). The dataset contains two observations per patient (the time to
first and second recurrence of infection). In addition there are three
covariates: \emph{age} (continuous), \emph{sex} (dichotomous) and \emph{type of
  disease} (categorical, four levels), and an individual-specific random effect
$u_i\sim N(0,\sigma^2)$. Thus, the linear predictor becomes
\[
  \eta_i = \beta_0 + \beta_\mathrm{sex}\,\cdot\mathrm{sex}_i +
  \beta_\mathrm{age}\,\cdot\mathrm{age}_i +
  \mathbf{\beta}_\mathrm{D}\,\mathbf{x}_i + u_i,
\]
where $\mathbf{\beta}_\mathrm{D}=(\beta_1,\beta_2,\beta_3)$ and $\mathbf{x}_i$
is a dummy vector coding for the disease type. Parameter estimates are shown in
the table below. 
\begin{center}
  \footnotesize
  \begin{tabular}{lrrrrrrrr}
    \hline
              & $\beta_0$ & $\beta_\mathrm{age}$ & $\beta_1$ & $\beta_2$ & $ \beta_3$ & $\beta_\mathrm{sex}$ & $r$    & $\sigma$\\
    \hline
    ADMB-RE    & -4.3440   & 0.0030               & 0.1208    & 0.6058    & -1.1423    & -1.8767              & 1.1624 & 0.5617  \\
    Std.\ dev. &  0.8720   & 0.0137               & 0.5008    & 0.5011    &  0.7729    &  0.4754              & 0.1626 & 0.2970  \\
    BUGS       & -4.6000   & 0.0030               & 0.1329    & 0.6444    & -1.1680    & -1.9380              & 1.2150 & 0.6374  \\
    Std.\ dev. &  0.8962   & 0.0148               & 0.5393    & 0.5301    &  0.8335    &  0.4854              & 0.1623 & 0.3570  \\
    \hline
  \end{tabular}
\end{center}
\paragraph{Files} http://otter-rsch.com/admbre/examples/kidney/kidney.html

\section{Block-diagonal Hessian}
This section contains models with grouped or nested random effects

\subsection{Nonlinear mixed models; a NLME comparison}
\label{sec:orange}
\paragraph{Model description}
The orange tree growth data was used by \citeasnoun[Ch.8.2]{pinh:bate:2000}
to illustrate how a logistic growth curve model with random effects can be
fit with the S-Plus function \texttt{nlme.} The data contain measurements made at
seven occasions for each of five orange trees:

\begin{center}
\begin{tabular}{ll}
$t_{ij}$ & Time point when the $j$th measurement was made on tree $i$ \\ 
$y_{ij}$ & Trunk circumference of tree $i$ when measured at time point $%
t_{ij}$%
\end{tabular}
\end{center}
The following logistic model is used:%
\[
y_{ij}=\frac{\phi_{1}+u_i}{1+\exp \left[ -\left( t_{ij}-\phi_{2}\right)
/\phi_{3}\right] }+\varepsilon_{ij}, 
\]%
where $(\phi_{1},\phi_{2},\phi_{3})$ are hyper-parameters, and $u_i\sim
N(0,\sigma_{u}^{2})$ is a random effect, and $\varepsilon_{ij}\sim
N(0,\sigma ^{2})$ is the residual noise term.

\paragraph{Results}
Parameter estimates are shown in the following table.
\begin{center}
\begin{tabular}{llllll}
& $\phi_{1}$ & $\phi_{2}$ & $\phi_{3}$ & $\sigma $ & $\sigma_{u}$ \\ 
\hline
ADMB-RE & 192.1 & 727.9 & 348.1 & 7.843 & 31.65 \\ 
Std. dev. & 15.658 & 35.249 & 27.08 & 1.013 & 10.26 \\ 
\texttt{nlme} & 191.0 & 722.6 & 344.2 & 7.846 & 31.48%
\end{tabular}
\end{center}
The difference between the estimates obtained with ADMB-RE and \texttt{nlme}
is small. The difference is caused by the fact that the two approaches use
different approximations to the likelihood function. (ADMB-RE uses the
Laplace approximation, and for \texttt{nlme} the reader is referred to %
\cite[Ch. 7]{pinh:bate:2000}.)

The computation time for ADMB was 0.58 seconds, while the computation time
for \texttt{nlme} (running under\ S-Plus 6.1) was 1.6 seconds.

\paragraph{Files} http://otter-rsch.com/admbre/examples/orange/orange.html

\newpage

\subsection{Pharmacokinetics; a NLME comparison}
\label{sec:pheno}

\paragraph{Model description}
The `one-compartment open model' is commonly used in pharmacokinetics. It
can be described as follows. A patient receives a dose $D$ of some substance
at time $t_{d}$. The concentration $c_t$ at a later time point $t$ is
governed by the equation%
\[
c_t=\frac{D}{V}\exp \left[ -\frac{Cl}{V}(t-t_{d})\right] 
\]%
where $V$ and $Cl$ are parameters (the so-called `Volume of concentration'
and the `Clearance'). Doses given at different time points contribute
additively to $c_t$. \citeasnoun[Ch.~6.4]{pinh:bate:2000} fitted this
model to a dataset using the S-Plus routine \texttt{nlme}. The linear
predictor used by \citeasnoun[p.~300]{pinh:bate:2000} is:%
\begin{eqnarray*}
\log \left( V\right) &=&\beta_{1}+\beta_{2}Wt+u_{V}, \\
\log \left( Cl\right) &=&\beta_{3}+\beta_{4}Wt+u_{Cl},
\end{eqnarray*}%
where $Wt$ is a continuous covariate, and $u_{V}\sim N(0,\sigma_{V}^{2})$
and $u_{Cl}\sim N(0,\sigma_{Cl}^{2})$ are random effects. The model
specification is completed by the requirement that the observed
concentration $y$ in the patient is related to the true concentration by $%
y=c_t+\varepsilon $, where $\varepsilon \sim N(0,\sigma ^{2})$ is a
measurement error term.

\paragraph{Results}

Estimates of hyper-parameters are shown in the following table:

\begin{center}
\begin{tabular}{llllllll}
& $\beta_{1}$ & $\beta_{2}$ & $\beta_{3}$ & $\beta_{4}$ & $\sigma $ & $%
\sigma_{V}$ & $\sigma_{Cl}$ \\ 
ADMB-RE & -5.99 & 0.622 & -0.471 & 0.532 & 2.72 & 0.171 & 0.227 \\ 
Std. Dev & 0.13 & 0.076 & 0.067 & 0.040 & 0.23 & 0.024 & 0.054 \\ 
\texttt{nlme} & -5.96 & 0.620 & -0.485 & 0.532 & 2.73 & 0.173 & 0.216%
\end{tabular}
\end{center}
The differences between the estimates obtained with ADMB-RE and \texttt{nlme}
are caused by the fact that the two methods use different approximations of
the likelihood function. ADMB-RE uses the Laplace approximation, while the
method used by \texttt{nlme} is described in %
\citeasnoun[Ch.~7]{pinh:bate:2000}.

The time taken to fit the model by ADMB-RE was 17 seconds, while the
computation time for \texttt{nlme} (under S-Plus 6.1) was 7 seconds.

\paragraph{Files} http://otter-rsch.com/admbre/examples/pheno/pheno.html

\newpage

\subsection{Frequency weighting in ADMB-RE}
\label{seq:frequency_example}

\paragraph{Model description}
Let $X_{i}$ be binomially distributed with paramters $N=2$ and $p_{i}$, and
assume that%
\begin{equation}
p_{i}=\frac{\exp (\mu +u_{i})}{1+\exp (\mu +u_{i})},
\end{equation}%
where $\mu $ is a parameter and $u_{i}\sim N(0,\sigma ^{2})$ is a random
effect. Assuming independe, the loglikelihood function for the parameter $%
\theta =(\mu ,\sigma )$ can be written:

\begin{equation}
l(\theta )=\sum_{i=1}^{n}\log \left[ p(x_{i};\theta )\right] .
\end{equation}%
In ADMB-RE $p(x_{i};\theta )$ is approximated using the Laplace
approximation. However, since $x_{i}$ only can take the values $0$, $1$ and $%
2$, we can re-write the loglikelihood as 
\begin{equation}
l(\theta )=\sum_{j=0}^{2}n_{j}\log \left[ p(j;\theta )\right] ,  \label{l_w}
\end{equation}%
where $n_{j}$ is the number $x_{i}$ being equal to $j$. Still the Laplace
approximation must be used to approximate $p(j;\theta )$, but now only for $%
j=0,1,2$, as opposed to $n$ times above. For large $n$ this can give large
savings.

To implement the loglikelihood (\ref{l_w}) in ADMB-RE you must organize your
code into a SEPARABLE\_FUNCTION (see the section ''Nested models'' in the
ADMB-RE manual). Then you should do the following

\begin{itemize}
\item Formulate the objective function in the weighted form (\ref{l_w}).

\item Include the statement \texttt{!! set\_multinomial\_weights(w)} in the
PARAMTER\_SECTION, where \texttt{w }is a vector (with indexes starting at 1)
containg the weights, so in our case $w=(n_{0},n_{1},n_{2})$.
\end{itemize}

\paragraph{Files} http://otter-rsch.com/admbre/examples/weights/weights.html

\newpage

\subsection{Ordinal logistic regression}
\label{sec:socatt_example}

\paragraph{Model description}
In this model the response variable $y$ takes on values from the ordered set
$\{y^{(s)},s=1,\ldots,S-1\}$, where $y^{(1)}<y^{(2)}<\cdots<y^{(S)}$. For
$s=1,\ldots,S-1$ define $P_s=P(y\leq y^{(s)})$ and $\kappa_s=\log
[P_s/(1-P_s)]$. To allow $\kappa_s$ to depend on covariates specific to the
$i$th observation ($i=1,\ldots,n$) we introduce a disturbance $\eta_i$ of
$\kappa_s$:
\[
  P(y_i\leq y^{(s)}) =
  \frac{\exp(\kappa_s-\eta_i)}
  {1+\exp(\kappa _{s}-\eta _{i})}, \qquad s=1,\ldots,S-1.
\]
with
\[
  \eta_i = \mathbf{X}_i\mathbf{\beta}+u_{j_i},
\]
where $\mathbf{X}_i$ and $\mathbf{\beta}$ play the sample role as in Example
1-3, the $u_j$ ($j=1,\ldots,q$) are independent $N(0,\sigma^2)$ variables, and
$j_i$ is the latent variable class of individual $i$.

\paragraph{Files} http://otter-rsch.com/admbre/examples/socatt/socatt.html

\newpage

\section{Banded Hessian (state-space)}
Examples of state-space models.

\subsection{Stochastic volatility models in finance}

\paragraph{Model description}
Stochastic volatility models are used in mathematical finance to
describe the evolution of asset returns, which typically exhibit changing variances over
time. As an illustration we use a time series of daily pound/dollar exchange rates $\{z_t\}$
from the period 01/10/81 to 28/6/85, previously analyzed by~\citeasnoun{harv:ruiz:shep:1994}.
The series of interest are the daily mean-corrected returns $\{y_t\}$, given by the
transformation
\[
  y_t = \log z_t - \log z_{t-1} - n^{-1}\sum_{i=1}^n(\log z_t-\log z_{t-1}).
\]

The stochastic volatility model allows the variance of $y_t$ to vary smoothly with time. This
is achieved by assuming that $yt\sim N(\mu,\sigma_t^2)$, where $\sigma_t^2=\exp(\mu_x+x_t)$. The smoothly
varying component $x_t$ follows the autoregression

\[
  x_t = \beta x_{t-1} + \varepsilon_t, \qquad \varepsilon_t \sim N(0,\sigma^2).
\]

The vector of hyper-parameters is for this model is thus $(\beta,\sigma,\mu,\mu_x)$.

\paragraph{Files} http://otter-rsch.com/admbre/examples/sdv/sdv.html

\newpage

\subsection{A discrete valued time series; The polio dataset}
\label{sec:sdv_example}

\paragraph{Model description}
\citeasnoun{zege:1988} analyzed a time series of monthly numbers of poliomyelitis cases
during the period 1970--1983 in the US. We make comparison to the performance of
the Monte Carlo Newton-Raphson method as reported in~\citeasnoun{kuk:chen:1999}. We
adopt their model formulation.

Let $y_{i}$ denote the number of polio cases in the $i$th period
$(i=1,\ldots,168)$. It is assumed that the distribution of $y_{i}$ is governed
by a latent stationary AR(1) process $\{u_i\}$ satisfying

\[
  u_i = \rho u_{i-1} + \varepsilon_i,
\]

where the $\varepsilon_i\sim N(0,\sigma^2)$ variables. To account for trend and
seasonality the following covariate vector is introduced

\[
  \mathbf{x}_i = \left(
    1,
    \frac{i}{1000},
    \cos\left(\frac{2\pi}{12}i\right),
    \sin\left(\frac{2\pi}{12}i\right),
    \cos\left(\frac{2\pi}{6}i\right),
    \sin\left(\frac{2\pi}{6}i\right)
  \right).
\]

Conditionally on the latent process $\{u_i\}$, the counts $y_i$ are
independently Poisson distributed with intensity

\[
  \lambda_i=\exp(\mathbf{x}_i{}'\mathbf{\beta}+u_i).
\]

Conditionally on the latent process $\{u_i\}$, the counts $y_i$ are
independently Poisson distributed with intensity

\[
  \lambda_i=\exp(\mathbf{x}_i{}'\mathbf{\beta}+u_i).
\]

\paragraph{Results}

Estimates of hyper-parameters are shown in the following table.


\begin{center}
  \footnotesize
  \begin{tabular}{lrrrrrrrr}
    \hline
    ~                   & $\beta_1$ & $\beta_2$ & $\beta_3$ & $\beta_4$ & $\beta_5$ & $\beta_6$ & $\rho$ & $\sigma$\\
    \hline
    ADMB-RE             & 0.242     & -3.81     & 0.162     & -0.482    & 0.413     & -0.0109   & 0.627  & 0.538   \\
    Std.\ dev.          & 0.270     &  2.76     & 0.150     &  0.160    & 0.130     &  0.1300   & 0.190  & 0.150   \\
    \citeasnoun{kuk:chen:1999} & 0.244     & -3.82     & 0.162     & -0.478    & 0.413     & -0.0109   & 0.665  & 0.519   \\
    \hline
  \end{tabular}
\end{center}

We note that not the standard deviation is large for several regression
parameters. The ADMB-RE estimates (which are based on the Laplace approximation)
very are very similar to the exact maximum likelihood estimates as obtained with
the method of~\citeasnoun{kuk:chen:1999}.

\paragraph{Files} http://otter-rsch.com/admbre/examples/polio/polio.html

\section{Generally sparse Hessian}

\subsection{Multilevel Rasch model}
The multilevel Rasch model can be implented using random effects in ADMB. As an example we use data on the responses of 
2042 soldiers to a total of 19 items (questions), taken from Doran et al (2007). This illustrates the use of crossed random effects in ADMB. 
Further, it is shown how the model easily can be generalized in ADMB. These more general models cannot be fitted with standard GLMM software 
such as "lmer" in R.

\paragraph{Files} http://admb-project.org/community/tutorials-and-examples/
random-effects-example-collection/item-response-theory-irt-and-the-multilevel-rasch-model-1


\chapter{Which ADMB features are not in ADMB-RE}
\begin{itemize}
\item Profile likelihoods cannot be used.
\item Certain functions, especially for matrix operations, have not been implemented.
\end{itemize}

You will find that not all the functionality of ordinary ADMB has yet been implemented 
in ADMB-RE. Functions are being added all the time.

\chapter{Command line options}
\label{sec:command_line_options}
\index{sec:command line options!ADMB-RE specific} A list of command line options accepted by ADMB programs can
be obtained using the command line option \texttt{-?}, for instance 
\begin{lstlisting}
    $ simple -?
\end{lstlisting}
Those options that are specific to ADMB-RE are printed after line the "Random effects options if applicable": 
\nopagebreak
\begin{center}
\begin{tabular}{ll}
\hline \textbf{Option}& \textbf{Explanation}\\ \hline
\texttt{-nr N} &           maximum number of Newton-Raphson steps\\
\texttt{-imaxfn N} &       maximum number of fevals in quasi-Newton inner optimization\\
\texttt{-is N} &           set importance sampling size to n for random effects\\
\texttt{-isf N} &          set importance sampling size funnel blocksto n for random effects\\
\texttt{-isdiag} &         print importance sampling diagnostics\\
\texttt{-hybrid} &         do hybrid Monte Carlo version of MCMC\\
\texttt{-hbf} &            set the hybrid bounded flag for bounded parameters\\
\texttt{-hyeps} &          mean step size for hybrid Monte Carlo\\
\texttt{-hynstep} &        number of steps for hybrid Monte Carlo\\
\texttt{-noinit} &         do not initialize random effects before inner optimzation\\
\texttt{-ndi N} &          set maximum number of separable calls\\
\texttt{-ndb N} &          set number of blocks for derivatives for random effects (reduces temporary file sizes)\\
\texttt{-ddnr} &           use high precision Newton-Raphson for inner optimization for banded hessian case ONLY even if implemented\\
\texttt{-nrdbg} &           verbose reporting for debugging newton-raphson\\
\texttt{-mm N} &          do minimax optimization\\
\texttt{-shess} &         use sparse Hessian structure inner optimzation\\
\hline
\texttt{-l1 N} & set the size of buffer \texttt{f1b2list1} to~\texttt{N}  \\
\texttt{-l2 N} & set the size of buffer \texttt{f1b2list12} to~\texttt{N}  \\
\texttt{-l3 N} & set the size of buffer \texttt{f1b2list13} to~\texttt{N}  \\
\texttt{-nl1 N} & set the size of buffer \texttt{nf1b2list1} to~\texttt{N} \\
\texttt{-nl2 N} & set the size of buffer \texttt{nf1b2list12} to~\texttt{N} \\
\texttt{-nl3 N} & set the size of buffer \texttt{nf1b2list13} to~\texttt{N} \\\hline
\end{tabular}
\end{center}
The last section (following the horisontal bar) are not printed, but can still be used (se earlier).


\chapter{Quick references}
\label{sec:quick}

\section{Compiling ADMB programs}
\vskip-1.5cm
\begin{figure}[H]
\includegraphics[scale=0.6]{compiling.pdf}
\label{fig:compiling}
\end{figure}

\nopagebreak
\section{}
\vskip0cm
\begin{figure}[H]
\includegraphics[width=17cm]{ADMBprim.pdf}
\label{fig:primer}
\end{figure}


\bibliographystyle{agsm}
\bibliography{skaug}
%\bibliography{Z:/tex/bib/skaug}


\printindex

\end{document}
